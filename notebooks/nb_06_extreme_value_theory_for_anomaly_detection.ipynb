{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "remove-input",
        "remove-output",
        "remove-input-nbconv",
        "remove-output-nbconv"
       ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_anomaly_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "remove-input",
        "remove-input-nbconv"
       ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "remove-input",
        "remove-output",
        "remove-input-nbconv",
        "remove-output-nbconv"
       ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "remove-input-nbconv",
        "remove-cell"
       ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_first_slide.svg\" alt=\"Snow\"\n",
    "style=\"width:100%;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'celluloid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Protocol, Sequence, Union, Tuple, List, TypeVar, Callable\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncAnimation\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcelluloid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Camera\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m     15\u001b[0m tfd \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mdistributions\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'celluloid'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from celluloid import Camera\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from typing import Protocol, Sequence, Union, Tuple, List, TypeVar, Callable\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extreme Value Theory and Anomaly Detection\n",
    "\n",
    "Extreme value theory (EVT) usually deals with tails of univariate probability distributions, i.e. with events that\n",
    "are rare because they are large.\n",
    "Anomaly detection deals with events that rare but not necessarily large. Therefore, the topic of anomaly detection is\n",
    "more general than EVT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Despite the smaller applicability of EVT techniques, they are still a valuable addition to the anomaly detectionist's\n",
    "toolbox. In several situations, anomalies directly correspond to large deviations from some (possibly running) mean - e.g. for sensor data, intrusion attacks based on the number of calls and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, even for entirely different definitions of anomaly, most detection algorithms will produce a scalar outlier score for each datapoint.\n",
    "EVT can then be used as a probabilistic framework for analyzing the univariate distribution of outlier scores and help determine meaningful thresholds for separating anomalous from normal cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EVT in a Nutshell\n",
    "\n",
    "There are two fundamental theorems of extreme value theory on which most results in that field are based. The first is concerned with the asymptotic distribution of block maxima of a sequence of i.i.d. random variables. The second one gives an expression for the distribution of excesses over a threshold.\n",
    "\n",
    "We will first state these theorems (in their standard formulation in the literature), then see how they can be applied to anomaly detection and after that highlight ideas of their proofs as well as some theoretical consequences.\n",
    "\n",
    "From now on let $X_1, X_2, ...$ be a sequence of 1-dimensional i.i.d. random variables with cumulative distribution function $F$. Let $X$ also be a r.v. with the same c.d.f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We define the _n-block maximum_ as the random variable\n",
    "\n",
    "$$\n",
    "M_n := \\max \\{X_1, ..., X_n\\}.\n",
    "$$\n",
    "\n",
    "Given a threshold $u$, _the excess over the threshold_ is given by $X-u$. \n",
    "\n",
    "In EVT, we are typically interested in approximating $P(M_n<z)$ for large $n$ and in approximating the distribution of excesses $P(X-u < y \\mid X > u)$ for large $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [Fisher-Tipett-Genedenko](https://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem) theorem characterizes the possible limits of renormalized block maxima.\n",
    "\n",
    "If there exist sequences of real numbers $a_n>0, b_n$ such that the probability distributions \n",
    "$$\n",
    "P\\left(\\frac{M_n-b_n}{a_n}<z \\right)\n",
    "$$\n",
    "converge to a _non-degenerate_ distribution $G(z)$, then $G(z)$ must be of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    " P\\left(\\frac{M_n-b_n}{a_n}<z \\right) \\xrightarrow[n\\rightarrow \\infty]{} G(z; \\xi, \\mu, \\sigma) = \\exp \\left\\{ -\\left( 1 + \\xi \\left( \\frac{z - \\mu}{\\sigma} \\right) \\right)^{- \\frac{1}{\\xi} } \\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "where $\\xi, \\mu \\in \\mathbb{R}$ and $\\sigma >0$. This function family is called the _Generalized Extreme Value distributions_ (GEV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [Pickands–Balkema–De Haan theorem](https://en.wikipedia.org/wiki/Pickands%E2%80%93Balkema%E2%80%93De_Haan_theorem) states that under the same conditions as above and for a threshold $u \\in \\mathbb{R}$ going to infinity, the distribution of _excesses over the threshold $u$_ converges to a _Generalized Pareto Distribution_ (GPD), i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "P(X-u < y \\mid X > u) \\xrightarrow[u \\rightarrow \\infty]{} H(y; \\xi, \\tilde{\\sigma})=1 - \\left( 1 + \\frac{\\xi \\ y}{\\tilde{\\sigma}} \\right)^{-\\frac{1}{\\xi}} \\ \n",
    "\\end{equation}\n",
    "\n",
    "where $y>0$ and $1 + \\frac{\\xi \\ y}{\\tilde{\\sigma}} >0$. The parameter $\\xi$ takes the same value as for the GEV.\n",
    "\n",
    "We have highlighted the dependence of the limiting distributions on the parameters in both cases. In applications, these parameters will be estimated based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Significance of EVT Theorems\n",
    "\n",
    "Before we analyze the consequences of the above distributions in detail, let us discuss their practical significance. The distinctive feature of the GEV and GPD distributions is that they are of a very restricted form, belonging to a three and two parameter function family respectively. This _motivates_ to model distributions of block maxima for finite but large $n$ by the GEV distribution\n",
    "\n",
    "$$\n",
    " P\\left(\\frac{M_n-b_n}{a_n}<z \\right) \\approx G(z; \\xi, \\mu, \\sigma) \\Longleftrightarrow \n",
    " P\\left( M_n < z \\right) \\approx G(z; \\xi, \\mu\\prime , \\sigma\\prime)\n",
    "$$\n",
    "\n",
    "where $\\mu\\prime=b_n+a_n \\mu$ and $\\sigma\\prime=a_n \\sigma$. Thus, fitting the coefficients $\\xi, \\mu\\prime, \\sigma\\prime$ to the observed values of $M_n$, e.g. by maximum likelihood estimation, also finds the \"best\" values of the renormalizing constants $a_n$ and $b_n$.\n",
    "\n",
    "Similarly, fitting $\\xi, \\tilde{\\sigma}$ to observed excesses of a finite threshold $u$ also finds the best renormalizing constants for the GPD. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "In the context of AD, modeling the distributions of $M_n$ or $X-u$ is useful for finding thresholds on outlier scores with probabilistic interpretations or for predicting the occurrence rates and sizes of anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, given some complex outlier score based on sensor data of a factory process, we might be interested in the probability that this outlier score exceeds a certain threshold within a month. This could be achieved by fitting a GEV distribution to observed frequencies of monthly maxima of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The GPD can be used to directly estimate a cumulative univariate distribution $F(z)$ for large enough $z$. Then one could use it to determine the anomaly threshold $z_{\\text{th}}$ by defining an anomalous upper quantile. E.g. solving $F(z_\\text{th}) = 0.99$ for $z_{\\text{th}}$ (where $F$ was obtained by fitting the GPD to some outlier score) would declare approximately 1% of data points as anomalous. We will describe this in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EVT in Action\n",
    "\n",
    "Let us give a quick example for fitting a GEV on data and extracting insight from it. For that we will use the NYC taxi calls dataset - a collection of taxi calls per 30-minutes intervals that was collected for over a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_csv = os.path.join(\"..\", 'data','nyc_taxi','nyc_taxi.csv')\n",
    "taxi_df = pd.read_csv(taxi_csv)\n",
    "taxi_df['time'] = [x.time() for x in pd.to_datetime(taxi_df['timestamp'])]\n",
    "taxi_df['date'] = [x.date() for x in pd.to_datetime(taxi_df['timestamp'])]\n",
    "taxi_df.rename(columns={\"value\": \"n_calls\"}, inplace=True)\n",
    "taxi_df.drop(columns=[\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for normalizing data. In most cases it will be enough to use the normalize function\n",
    "\n",
    "def normalize_data(data: Sequence) -> np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "def normalize_series(series: pd.Series) -> pd.DataFrame:\n",
    "    data = series.values.reshape(-1, 1)\n",
    "    normalized_data = normalize_data(data).reshape(-1)\n",
    "    return pd.Series(normalized_data, index=series.index)\n",
    "\n",
    "\n",
    "def normalize_df(data_frame: pd.DataFrame):\n",
    "    normalized_data = normalize_data(data_frame)\n",
    "    return pd.DataFrame(normalized_data, columns=data_frame.columns, index=data_frame.index)\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def normalize(data: T) -> T:\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return normalize_data(data)\n",
    "    elif isinstance(data, pd.Series):\n",
    "        return normalize_series(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        return normalize_df(data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data type: {data.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "taxi_df_normalized = taxi_df\n",
    "taxi_df_normalized[\"n_calls\"] = normalize(taxi_df_normalized[\"n_calls\"])\n",
    "taxi_df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can define a trainable GEV with tensorflow probability as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_gev(xi: float, mu=0., sigma=1., trainable_xi=True):\n",
    "    xi, mu, sigma = np.array([xi, mu, sigma]).astype(float)\n",
    "    if trainable_xi:\n",
    "        xi = tf.Variable(xi, name=\"xi\")\n",
    "    return tfd.GeneralizedExtremeValue(\n",
    "        loc=tf.Variable(mu, name='mu'),\n",
    "        scale=tf.Variable(sigma, name='sigma'),\n",
    "        concentration=xi,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A glance at the tensorflow probability API\n",
    "\n",
    "For solving the exercises in this notebook you will need to use basic properties of tensorflow probability distributions. They have a very intuitive and convenient API - you get access to the probability density, cdf, quantile function and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_gev = get_gev(0.5)\n",
    "\n",
    "print(f\"Probability density: {sample_gev.prob([1, 0.3])}\")\n",
    "print(f\"Cdf: {sample_gev.cdf([1, 0.3])}\")\n",
    "print(f\"Quantile: {sample_gev.quantile([0.5, 0.9])}\")\n",
    "print(f\"Trainable vars:\\n {sample_gev.trainable_variables}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1: playing around with GEV parameters\n",
    "\n",
    "Plot the GEV probability distribution for different values of $\\xi, \\mu$ and $\\sigma$. How do they differ qualitatively? \n",
    "\n",
    "What are the domains of definition of $z$ in the above analytic expression for $G(z)$? What values should the c.d.f. $G(z)$ take outside these domains and how does this affect fitting  $\\xi, \\mu, \\sigma$ from data by maximum likelihood estimation?\n",
    "\n",
    "What expression for the GEV do we get in the limit $\\xi \\longrightarrow 0$?\n",
    "\n",
    "The three qualitatively different shapes of the GEV have their own names. For $\\xi >0$ we get the [Fréchet Distribution](https://en.wikipedia.org/wiki/Fr%C3%A9chet_distribution), for $\\xi<0$ the reverse [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution) and for $\\xi=0$ the [Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution). Note that using the Gumbel distribution in tensorflow probability is not exactly the same as using GEV with $\\xi=0$ due to rounding errors. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gev = get_gev(xi=1e-5, sigma=2)\n",
    "arr = np.linspace(-5, 5)\n",
    "\n",
    "pdf = gev.prob(arr)\n",
    "plt.plot(arr, pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution Exercise 1:\n",
    "\n",
    "The cdf of the GEV distribution is well defined when $1 + \\xi \\left( \\frac{z - \\mu}{\\sigma} \\right) > 0$. This is equivalent to\n",
    "\n",
    "$$\n",
    " z > \\mu - \\frac{\\sigma}{\\xi} \\qquad \\text{if $\\xi>0$}\n",
    "$$\n",
    "and\n",
    "$$\n",
    " z < \\mu - \\frac{|\\sigma|}{\\xi} \\qquad \\text{if $\\xi<0$}.\n",
    "$$\n",
    "\n",
    "Thus, for $\\xi>0$, the distribution has a left boundary, the probability of points lying to the left of it is zero. The value of the cdf there is zero.\n",
    "\n",
    "For $\\xi<0$ there is a right boundary, the probability of points lying to the right of it is zero and the value of the cdf is 1. \n",
    "\n",
    "As $\\xi$ moves to zero from below, the right boundary is pushed to infinity. Similarly, if it approaches zero from above, the left boundary is pushed to negative infinity. At exactly $\\xi=0$, the GEV becomes the Gumbel distribution which is well defined for all $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can group the numbers of calls according to the dates, thereby obtaining daily maxima and minima of calls. One way of detecting anomalies in the NYC taxi data set is by fitting a GEV to the distribution of these daily maxima. Here a histogram plot of the (normalized) maxima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "daily_grouped = taxi_df.groupby(\"date\")[\"n_calls\"].agg([\"max\", \"min\", \"sum\"])\n",
    "daily_grouped[\"diff\"] = daily_grouped[\"max\"] - daily_grouped[\"min\"]\n",
    "daily_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "daily_grouped_normalized = normalize(daily_grouped)\n",
    "daily_grouped_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(daily_grouped_normalized[\"max\"], density=True, bins=40)\n",
    "plt.title(\"Daily maxima of n_calls/(30 minutes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Q__: Can you already spot the obvious anomalies? What caused them?\n",
    "\n",
    "__A__: See below\n",
    "\n",
    "\n",
    "__Q__: Which of the three qualitatively different shapes would make \"physical\" sense for the taxi calls data?\n",
    "\n",
    "__A__: The Weibull shape - thus we expect $\\xi<0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "maxima = daily_grouped_normalized[\"max\"]\n",
    "maxima[(maxima > 1.8) | (maxima < -3.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 02/11 - NY marathon\n",
    "- 25/12 - Christmas\n",
    "- 27/01 - Snowstorm\n",
    "- 01/01 - New Years\n",
    "- 06/09 - Columbus day (big parade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting the GEV\n",
    "\n",
    "Now let us infer the parameters of the GEV from the data using maximum likelihood estimation. We will make gradient descent on the negative log likelihood of the GEV. Here a very simple training loop for a suitable initial choice for the shape parameter $\\xi$ (it is called \"concentration\")  written out in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we are going to be a bit fancy and show an animation of the function as it is being fitted\n",
    "\n",
    "daily_max = daily_grouped_normalized[\"max\"].values\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=2e-4)\n",
    "losses = []\n",
    "\n",
    "sample_gev = get_gev(xi=-0.1, trainable_xi=True)\n",
    "\n",
    "fig = plt.figure(dpi=200, figsize=(4.5, 3))\n",
    "camera = Camera(fig)\n",
    "\n",
    "for step in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = - tf.math.reduce_sum(sample_gev.log_prob(daily_max))\n",
    "    gradients = tape.gradient(loss, sample_gev.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sample_gev.trainable_variables))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    bins = plt.hist(daily_max, bins=40, density=True, color=\"C0\")[1]\n",
    "    pdf = sample_gev.prob(bins)\n",
    "    plt.plot(bins, pdf, color=\"orange\")\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.5, 1.01, f\"{step=}, Loss={loss}\", transform=ax.transAxes)\n",
    "    camera.snap()\n",
    "\n",
    "plt.close()\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Negative Log Likelihood\")\n",
    "plt.xlabel(\"gradient steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Seems like after 100 steps we have already converged. Let us have a quick look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bin_positions = plt.hist(daily_max, density=True, bins=25)[1]\n",
    "plt.plot(bin_positions, sample_gev.prob(bin_positions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gev.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Well, we probably can do better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2.1: MLE for the generalized extreme value distribution\n",
    "\n",
    "Find a better fit using: \n",
    "  1. Removing the obvious anomalies\n",
    "  2. Profiling in the shape parameter $\\xi$ or using different initial values/learning rates for inferring $\\xi$\n",
    "\n",
    "Feel free to improve the code by defining new functions and so on!\n",
    "\n",
    "Evaluate the quality of your fit by visual inspection of a histogram and a Q-Q plot (more on that below).\n",
    "\n",
    "You can also use other statistical tools that you are familiar with.\n",
    "\n",
    "Use the fitted model to find \"anomalies\" for taxi calls corresponding to probabilities of less than $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-Q Plot\n",
    "\n",
    "A Q-Q plot is useful for visually comparing two distributions, or comparing a distribution with a dataset. We are interested in the latter. In a Q-Q plot the quantiles of one distribution are plotted against the quantiles of another. For a dataset, the natural choice of quantiles is given simply by the sorted data itself. The data points then roughly correspond to the $\\frac{k}{n+1}$th percentiles, where  $n$ is the number of samples and $k=1,...,n$ (these are often called _plotting positions_ and other choices for them are possible). The corresponding _theoretical quantiles_ from some specified c.d.f. $F$ are then given by $q_k \\ \\text{s.t} \\ F(q_k) = \\frac{k}{n+1}$ (in our applications, $F$ will generally be injective and the $q_k$ uniquely defined). \n",
    "\n",
    "\n",
    "If the distribution is a good fit of the data, the resulting line will be close to the diagonal. Below we ask you to complete a simple implementation of the Q-Q plot for tensorflow-like distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ArrayLike = Sequence[Union[float, tf.Tensor]]\n",
    "\n",
    "\n",
    "class TFDistributionProtocol(Protocol):\n",
    "    name: str\n",
    "    trainable_variables: Tuple[tf.Variable]\n",
    "        \n",
    "    def quantile(self, prob: ArrayLike) -> ArrayLike: ...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def qqplot(data: ArrayLike, dist: TFDistributionProtocol):\n",
    "    num_observations = len(data)\n",
    "    observed_quantiles = sorted(data)\n",
    "    plotting_positions = np.arange(1, num_observations + 1) / (num_observations + 1)\n",
    "    theoretical_quantiles = dist.quantile(plotting_positions)\n",
    "    \n",
    "    plot_origin = (theoretical_quantiles[0], observed_quantiles[0])\n",
    "    plt.plot(theoretical_quantiles, observed_quantiles)\n",
    "    plt.plot(theoretical_quantiles, theoretical_quantiles) # adding a diagonal for visual comparison\n",
    "    plt.xlabel(f\"Theoretical quantiles of {dist.name}\")\n",
    "    plt.ylabel(f\"Observed quantiles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Solution of exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# setting up functions for normal and profile likelihood fit\n",
    "\n",
    "def fit_dist(data: ArrayLike, dist: TFDistributionProtocol, num_steps=100, lr=1e-4, \n",
    "             plot_losses=True, return_animation=True) -> Union[float, Tuple[float, HTML]]:\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "    losses = []\n",
    "    \n",
    "    if return_animation:\n",
    "        fig = plt.figure(dpi=200, figsize=(4.5, 3))\n",
    "        camera = Camera(fig)    \n",
    "\n",
    "    for step in range(num_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = - tf.math.reduce_sum(dist.log_prob(data))\n",
    "        if np.isnan(loss.numpy()):\n",
    "            logging.warning(f\"Encountered nan after {step} steps\")\n",
    "            break\n",
    "        \n",
    "        gradients = tape.gradient(loss, dist.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, dist.trainable_variables))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if return_animation:\n",
    "            bins = plt.hist(data, bins=50, density=True, color=\"C0\")[1]\n",
    "            pdf = dist.prob(bins)\n",
    "            plt.plot(bins, pdf, color=\"orange\")\n",
    "            ax = plt.gca()\n",
    "            ax.text(0.5, 1.01, f\"{step=}, Loss={round(loss.numpy(), 2)}\", transform=ax.transAxes)\n",
    "            camera.snap()\n",
    "    \n",
    "\n",
    "    if plot_losses:\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Negative Log Likelihood\")\n",
    "        plt.xlabel(\"gradient steps\")\n",
    "        plt.show()\n",
    "    \n",
    "    result = losses[-1]\n",
    "    if return_animation:\n",
    "        result = result, HTML(camera.animate().to_html5_video())\n",
    "    return result\n",
    "\n",
    "def profile_fit_dist(data: ArrayLike, dist_factory: Callable[[float], TFDistributionProtocol], xi_values: Sequence[float], \n",
    "                     num_steps=100, lr=1e-4) -> Tuple[float, TFDistributionProtocol]:\n",
    "    \"\"\"\n",
    "    Fits the distribution to data and returns the final loss. If return_animation=True, returns the tuple\n",
    "    (final_loss, animation)\n",
    "    \"\"\"\n",
    "    minimal_loss = np.infty\n",
    "    optimal_dist = None\n",
    "    for xi in xi_values:\n",
    "        dist = dist_factory(xi)\n",
    "        loss = fit_dist(data, dist, num_steps=num_steps, lr=lr, plot_losses=False, return_animation=False)\n",
    "        if loss < minimal_loss:\n",
    "            minimal_loss = loss\n",
    "            optimal_dist = dist\n",
    "    if optimal_dist is None:\n",
    "        raise RuntimeError(f\"Could not find optimal dist, probably due to divergences during fit. \"  \n",
    "                           \"Try to find a better choice for xi_values\")\n",
    "    return minimal_loss, optimal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# removing obvious anomalies\n",
    "daily_max_without_anomalies = daily_max[np.logical_and( daily_max < 1.8, daily_max > -3.5)]\n",
    "\n",
    "plt.hist(daily_max_without_anomalies, density=True, bins=40)\n",
    "plt.title(\"Daily maxima without anomalies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Example with profile likelihood\n",
    "\n",
    "xi_values = np.linspace(-0.3, -0.5, 30)\n",
    "dist_factory = lambda xi: get_gev(xi, trainable_xi=False)\n",
    "min_loss, optimal_gev = profile_fit_dist(daily_max_without_anomalies, dist_factory, xi_values, num_steps=80)\n",
    "print(f\"Minimal loss: {min_loss}\")\n",
    "print(f\"Optimal xi: {optimal_gev.concentration}\")\n",
    "optimal_gev.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "bin_positions = plt.hist(daily_max_without_anomalies, density=True, bins=40)[1]\n",
    "plt.plot(bin_positions, optimal_gev.prob(bin_positions))\n",
    "plt.title(\"Result from profile likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Solving with gradient descent on xi\n",
    "daily_max_gev = get_gev(xi=-0.4)\n",
    "final_loss = fit_dist(daily_max_without_anomalies, daily_max_gev, return_animation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Here the values found by fitting\n",
    "daily_max_gev.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# and here the qqplot\n",
    "qqplot(daily_max_without_anomalies, daily_max_gev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Solution exercise 2.1 - Finding anomalies from the GEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#The fit looks quite good, apart from the lower region, which we are not really interested in. \n",
    "#Let us find the anomalies corresponding to the upper 1% quantile\n",
    "upper_percentile = 0.99\n",
    "upper_quantile = daily_max_gev.quantile(upper_percentile).numpy()\n",
    "upper_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# and here the anomalies above this threshold\n",
    "daily_grouped_normalized[\"max\"][daily_grouped_normalized[\"max\"] > upper_quantile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In addition to the obvious anomalies found above, we caught the independence day (one day before it). We also have the probabilistic interpretation that for 99% of the days the maximal amount of calls per 30 minutes will not exceed the threshold found above (it should be rescaled back to the original value for this statement to hold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating the uncertainty\n",
    "\n",
    "One benefit of the probabilistic approach is that we get confidence intervals almost for free. These can be used to estimate the robustness of our analysis (e.g. the determination of anomalies and the quality of the fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we fitted our functions using MLE, which is known to be approximately normal, we get uncertainty estimates from the second derivatives of the loss function. Fortunately, tensorflow makes this extremely easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def observed_fisher_information(data: ArrayLike, dist: TFDistributionProtocol) -> tf.Tensor:\n",
    "    with tf.GradientTape() as t2:\n",
    "        with tf.GradientTape() as t1:\n",
    "            nll = - tf.math.reduce_sum(dist.log_prob(data))\n",
    "        # conversion needed b/c trainable_vars is a tuple, so gradients and jacobians are tuples too\n",
    "        g = tf.convert_to_tensor(  \n",
    "            t1.gradient(nll, dist.trainable_variables)\n",
    "        )\n",
    "    return tf.convert_to_tensor(t2.jacobian(g, dist.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mle_std_deviations(data: ArrayLike, dist: TFDistributionProtocol) -> tf.Tensor:\n",
    "    observed_information_matrix = observed_fisher_information(data, dist)\n",
    "    mle_covariance_matrix = tf.linalg.inv(observed_information_matrix)\n",
    "    variances = tf.linalg.tensor_diag_part(mle_covariance_matrix)\n",
    "    return tf.math.sqrt(variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "\n",
    "## Exercise 2.2: Uncertainty in GEV \n",
    "\n",
    "Using the above functions, include error bars into the Q-Q plots of the maximum likelihood estimates of the GEV distribution found above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "## Solution Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# finding the stddevs and adding/substracting them from the values found from fitting\n",
    "std_devs = mle_std_deviations(daily_max_without_anomalies, daily_max_gev)\n",
    "print(f\"Found std_devs: {std_devs}\")\n",
    "\n",
    "coeff_fitted = tf.convert_to_tensor(daily_max_gev.trainable_variables)\n",
    "coeff_upper = coeff_fitted + std_devs\n",
    "coeff_lower = coeff_fitted - std_devs\n",
    "\n",
    "# creating GEVs corresponding to the boundaries of the confidence intervals found above\n",
    "gev_upper = get_gev(*coeff_upper)\n",
    "gev_lower = get_gev(*coeff_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# The qqplots for the original GEV and the GEVs at the boundaries\n",
    "\n",
    "qqplot(daily_max_without_anomalies, daily_max_gev)\n",
    "qqplot(daily_max_without_anomalies, gev_upper)\n",
    "qqplot(daily_max_without_anomalies, gev_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 3: GEV for minima\n",
    "\n",
    "Now let us repeat the same analysis fitting the distribution of the daily minima using the same strategy. Since minima for a univariate random variable $X$ correspond to maxima of $-X$, all we have to do is to fit a GEV to the minima multiplied by -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "## Solution of exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "neg_minima_series = -daily_grouped_normalized[\"min\"]\n",
    "neg_daily_min = neg_minima_series.values\n",
    "\n",
    "plt.hist(neg_daily_min, density=True, bins=40)\n",
    "plt.title(\"Daily minima * (-1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# identifying obvious anomalies\n",
    "neg_minima_series[(neg_minima_series>2) | (neg_minima_series<-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# - 01/01 - New Year\n",
    "# - 01-02/11 - Marathon\n",
    "# - 26-27/01 - Snowstorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "neg_minima_without_anomalies = neg_daily_min[np.logical_and(neg_daily_min<2, neg_daily_min>-2)]\n",
    "plt.hist(neg_minima_without_anomalies, density=True, bins=40)\n",
    "plt.title(\"Daily minima * (-1) without obvious anomalies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "daily_min_gev = get_gev(xi=-0.3)\n",
    "final_loss = fit_dist(neg_minima_without_anomalies, daily_min_gev, return_animation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "qqplot(neg_minima_without_anomalies, daily_min_gev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Fit looks good in the region we are interested in, let us find the 1% quantile and the corresponding anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "upper_quantile = daily_min_gev.quantile(0.99).numpy()\n",
    "upper_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "neg_minima_series[neg_minima_series>upper_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Only one non-obvious anomaly is found in the upper quantile, it is cause by the snowstorm responsible for the obvious anomalies we have seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison with Z-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "daily_means = daily_grouped_normalized[\"sum\"]\n",
    "\n",
    "plt.plot(daily_means.values)\n",
    "plt.axhline(y=2., color='r', linestyle='-')\n",
    "plt.axhline(y=-2., color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The big question here is: where to put the threshold? Clearly the assumption of a Gaussian distribution underlying the sum of daily calls is incorrect - the distribution seems skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(daily_means, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can detect some anomalies with the Z-test, of course, but the probabilistic interpretation is going to be flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_means[np.abs(daily_means) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A look back at the theory\n",
    "\n",
    "So, what have we really done and why does it make sense to use the GEV for such problems? What kind of guarantees does the Fisher-Tipett-Gnedenko theorem give us about the quality of the fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Well, the truth is, not too many. First notice the following exact equality:\n",
    "\n",
    "$$\n",
    "P(M_n < z) = P(X_1< z \\text{ and } X_2 < z ... \\text{ and  } X_n < z) = F^n(z)\n",
    "$$\n",
    "\n",
    "So, if we know the cumulative distribution, there is no need to resort to the GEV. Typically, of course, we do not know it. The above equality implies:\n",
    "\n",
    "$$\n",
    "\\lim P(M_n < z) =     \n",
    "    \\begin{cases}\n",
    "      0 & \\text{if}\\ F(z) < 1 \\\\\n",
    "      1 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We actually always know the exact limit of the distribution of the block-maxima! It is degenerate (either a step function of identical zero). In fact, this degenerate distribution can be seen as a limit of the GEV. It would correspond to normalizing constants $a_n=1, \\ b_n=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While this observation is very simple and the difference between the cdf of block maxima $P(M_n < z)$ and its degenerate limit does decrease as $n$ increases, this limiting distribution is unexpressive and fitting it to data does not provide probabilistic insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Q__: How many parameters does the exact limit of $F^n$ have? What would we get if we fit it to data?\n",
    "\n",
    "__A__: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introducing the normalizing constants $a_n$ and $b_n$ _might_ allow the distribution of renormalized block maxima to converge to something non-trivial. It also might not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In applications we usually care about modeling $M_n$ for a _fixed $n_0$_ (or maybe for a few selected $n_i$). An arbitrary series of $a_n$ and $b_n$ that at some point helps convergence does not directly address our needs. In fact, this is also not what we do - by fitting the GEV parameters to data for our selected $n_0$ we automatically find the _best_ $a_{n_0}$ and $b_{n_0}$ that minimize the difference between $F^{n_0}(z)$ and $G(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly $G(z)$ is much more expressive than the degenerate exact limit and could potentially provide a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, the convergence that we really care about is to answer the question:\n",
    "\n",
    "\n",
    "How well do the best fits of $G(z)$ for fixed $n$ - let us call them $G_n(z)$ - approximate the distributions $F^n(z)$ as $n$ increases? One could e.g. be interested in the infinity norm\n",
    "\n",
    "$$\n",
    "\\Delta_n := \\sup_z | F^n(z) - G_n(z) |\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is not the same as asking how well $G(z)$ approximates some rescaled variant of $F^n(z)$ with $n$-dependent normalization constants! That would be\n",
    "\n",
    "$$\n",
    "\\tilde{\\Delta}_n(a_n, b_n) := \\sup_z |F^n(a_n z + b_n) - G(z) |\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In the latter question, the choice of normalization constants matters, in the former it does not - they are implicitly determined by the best fit for each $n$. Since for $\\Delta_n$ the $a_n, b_n$ have been optimized, one could reasonably expect a relation of the type\n",
    "\n",
    "$$\n",
    "\\Delta_n \\approx \\min_{a_n, b_n} \\tilde{\\Delta}_n(a_n, b_n)\n",
    "$$\n",
    "\n",
    "to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is easy to see that given some normalizing sequences $a_n, b_n$, the convergence to a GEV is possible, than with other sequences $\\tilde{a}_n, \\tilde{b}_n$ with some $a>0, b$ such that\n",
    "\n",
    "$$\n",
    "\\lim_{n\\rightarrow \\infty} \\frac{\\tilde{a}_n}{a_n} = a \\quad,\\quad \\lim_{n \\rightarrow \\infty} \\frac{b_n-\\tilde{b}_n}{a_n} = b\n",
    "$$\n",
    "\n",
    "the rescaled $\\frac{M_n-\\tilde{b}_n}{\\tilde{a_n}}$ also converges to a GEV of the same type (with the same $\\xi$). This is often formulated that a distribution $F$ has  _a fixed domain of attraction_. However, the error rates $\\tilde{\\Delta}_n(\\tilde{a}_n, \\tilde{b}_n)$ would be different from those associated to $a_n, b_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unfortunately, theoretical bounds for the quantity of interest $\\Delta_n$ are hard to come by - we are not aware of any. They also highly depend on the fitting procedure, which is non-trivial, as we have seen above. There are some bounds for quantities of the type $\\tilde{\\Delta}_n(\\tilde{a}_n, \\tilde{b}_n)$ (see the annotated literature reference) but they are rather loose and not really helpful in practice. Therefore, the EVT theorems are more of a motivation for selecting distribution families for fitting than a rigorous approach with guarantees. In practice the convergence and fit tend to work pretty well, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 4 (theoretical, bonus): outlining the proof of the Fisher-Gnedenko-Tripet theorem\n",
    "\n",
    "One may wonder how the statement of the Fisher-Gnedenko-Tripet theorem is obtained without providing bounds on convergence. The reason is that the limiting distribution of (renormalized) maxima must have a very special property - it must be max stable. It is instructive to go through a part of the proof to get a feeling for the EVT theorems. We will do so in this exercise.\n",
    "\n",
    "\n",
    "__Definition__: A cumulative distribution function $D(z)$ is called _max-stable_ iff for all $n\\in\\mathbb{N} \\ \\exists \\ \\alpha_n>0, \\beta_n \\in  \\mathbb{R}$ such that \n",
    "\n",
    "$$\n",
    "D^n(z) = D(\\alpha_n z + \\beta_n)\n",
    "$$\n",
    "\n",
    "Prove that from $\\lim_{n\\rightarrow \\infty} P\\left( \\frac{M_n - b_n}{a_n} < z \\right) = G(z)$ follows that $G(z)$ is max-stable.\n",
    "\n",
    "This goes a long way towards proving the first EVT theorem. One can easily compute that the GEV distribution is max-stable and with more effort one can also prove that any max-stable distribution belongs to the GEV family. Thus, the proof of the theorem is very implicit and does not involve any convergence rates or bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 5: increase the block size\n",
    "\n",
    "According to the line of thought above, increasing the block-size before determining the maxima should improve convergence. Of course, it also decreases the number of points for fitting so it increases variance. We will analyze uncertainties of the fitted GEV below.\n",
    "\n",
    "Repeat the fit of the GEV for 2-day maxima/minima. What do you think about the result?\n",
    "\n",
    "_Hint: use the .reshape method of numpy arrays on the already computed daily maxima/minima_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "## Solution exercise 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "bidaily_maxima = daily_max_without_anomalies.reshape(-1, 2).max(axis=1)\n",
    "\n",
    "plt.hist(bidaily_maxima, bins=40)\n",
    "plt.title(\"Bidaily maxima\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "bidaily_gev = get_gev(xi=-0.5)\n",
    "loss = fit_dist(bidaily_maxima, bidaily_gev, lr=3e-4, num_steps=100, return_animation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "bidaily_gev.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "source": [
    "The shape parameter should be independent of the size of the block (it is not affected by $a_n$ and $b_n$) .\n",
    "Of course, since we find it from fitting, we shouldn't be surprised to find a slightly different value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "source": [
    "We get a better fit than before (less than half of the loss with half as many data points). \n",
    "But we have higher variance in the very important shape parameter $\\xi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "std_devs_daily = mle_std_deviations(daily_max_without_anomalies, daily_max_gev)\n",
    "std_devs_bidaily = mle_std_deviations(bidaily_maxima, bidaily_gev)\n",
    "\n",
    "print(\"Daily stddevs:\")\n",
    "print(std_devs_daily.numpy())\n",
    "print(\"Biaily stddevs:\")\n",
    "print(std_devs_bidaily.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Peaks over threshold (PoT)\n",
    "\n",
    "So far we have only used the first theorem of EVT. As you might have noticed above, it can be somehow wasteful when it comes to data efficiency. Since the GEV is fitted on block-maxima, a huge number of data points remain unused for parameter estimation. The second theorem of EVT gives rise to a more efficient approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise 6 (theoretical, bonus): deriving the second theorem of EVT\n",
    "\n",
    "Use the approximation $\\ln(1+x) \\approx 1 + x$ for $|x| \\ll 1$ and $F(z) \\approx 1$ for large enough $z$ to derive.\n",
    "\n",
    "\\begin{equation}\n",
    "P(X-u < y \\mid X > u) \\approx 1 - \\left( 1 + \\frac{\\xi \\ y}{\\tilde{\\sigma}} \\right)^{-\\frac{1}{\\xi}} \\label{GPD-approx-original}\n",
    "\\end{equation}\n",
    "\n",
    "for large enough $u$ (this is a slightly less formal derivation of Pickards' et. al. theorem). One could equivalently write\n",
    "\n",
    "\\begin{equation}\n",
    "P(X-u > y \\mid X > u) \\approx \\left( 1 + \\frac{\\xi \\ y}{\\tilde{\\sigma}} \\right)^{-\\frac{1}{\\xi}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "What is the relation between $\\tilde{\\sigma}$ and the normalizing coefficients of the first theorem of EVT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The above equation can be used to estimate the entire tail of the cdf $F$ of $X$ from a sample of size $N$ obtained by sampling repeatedly from $F$. First note that for a single $u$ we can approximate the cdf through the sample statistics as:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "1-F(u) = P(X>u) \\approx \\frac{N_u}{N}\n",
    "\\end{equation}\n",
    "\n",
    "where $N_u$ is the number of samples with values above $u$. Interpreting $u$ as a threshold, we will call those samples _peaks over threshold_ (PoT) and $N_u$ is simply their count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Q__: What should $u$ and the data set fulfill in order for the above approximation to be accurate?\n",
    "\n",
    "__A__: It should be small enough such that many data points are larger than it. Then the approximation in $P(X>u) \\approx \\frac{N_u}{N}$ holds (the estimator is not too biased). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can perform a series of approximations for $z>u$ to get to the tail-distribution. First using $P(X>u) \\approx \\frac{N_u}{N}$ we get\n",
    "\n",
    "$$\n",
    "P(X>z) = P(X>z \\cap X>u) = P(X>z \\mid X>u) P(X>u) \\approx \\frac{N_u}{N} P(X>z \\mid X>u)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we use the GDP theorem to approximate\n",
    "\n",
    "$$\n",
    "P(X>z \\mid X>u) = P(X-u > z -u \\mid X>u) \\approx\n",
    "    \\left( 1 + \\frac{\\xi (z-u)}{\\tilde{\\sigma}} \\right)^{-\\frac{1}{\\xi}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Putting everything together gives\n",
    "\n",
    "$$\n",
    "P(X>z) \\approx \\frac{N_u}{N}  \\left( 1 + \\frac{\\xi (z-u)}{\\tilde{\\sigma}} \\right)^{-\\frac{1}{\\xi}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Q__: Intuitively, what does $u$ need to fulfill for both approximations to hold?\n",
    "\n",
    "__A__: $u$ should be small enough such that the approximation $P(X>u) \\approx \\frac{N_u}{N}$ holds and sufficiently large such that the generalized pareto distribution is a good estimate of the tail of the distribution for values larger than $u$. Intuitively, it should be at the *beginning of the tail*, where for values larger than $u$ only the tail behavior plays a role - i.e. no more local extrema or other specifics of the underlying distribution of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 7: Using the GPD for anomaly detection\n",
    "\n",
    "This exercise lets you explore the second theorem of EVT for anomaly detection. Here we let you calculate and code on your own, without giving too many hints. You can follow the GEV-fitting code above for solving this exercise. Feel free to ask for hints if you are stuck!\n",
    "\n",
    "1. Using the results above, find an approximation of the upper quantile $z_q$ such that $P(X>z_q) < q$ (assuming $z_q > u$).\n",
    "2. What is the relation of this quantile to the quantile of the generalized pareto distribution?\n",
    "3. Select a threshold $u$ and fit the generalized pareto distribution to the peaks over this threshold using tensorflow-probability and the same tricks that were used above for fitting the GEV distribution. You might want to use the profile likelihood fitting.\n",
    "4. Determine anomalies from the quantile function.\n",
    "5. What advantages do you see in fitting the GPD with PoT compared to fitting GEV distribution using block-maxima for anomaly detection? What are the disadvantages?\n",
    "6. Check the quality of your fit and perform an uncertainty analysis as above for the GEV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "## Solution Exercise 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# We define the creation of the GPD analogous to the GEV above\n",
    "\n",
    "def get_gpd(xi: float,  sigma=1., trainable_xi=True):\n",
    "    xi, sigma = np.array([xi, sigma]).astype(float)\n",
    "    if trainable_xi:\n",
    "        xi = tf.Variable(xi, name=\"xi\")\n",
    "    return tfd.GeneralizedPareto(\n",
    "        loc=0,\n",
    "        scale=tf.Variable(sigma, name='sigma'),\n",
    "        concentration=xi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# GPD is fit directly on the thresholded data, no need for grouping\n",
    "\n",
    "n_calls = taxi_df_normalized[\"n_calls\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "plt.hist(n_calls, bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# seems like u=1 gives a good value for the beginning of \"tail behaviour\"\n",
    "\n",
    "u = 1\n",
    "thresholded_n_calls = n_calls[n_calls>u] - u\n",
    "plt.hist(thresholded_n_calls, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# obvious anomalies\n",
    "taxi_df_normalized[taxi_df_normalized[\"n_calls\"]> u+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# we filter out some calls on (one day before) independence day, Columbus day, the marathon and New Year\n",
    "\n",
    "cleaned_thresholded_calls = thresholded_n_calls[thresholded_n_calls < 1]\n",
    "plt.hist(cleaned_thresholded_calls, bins=50, density=True)\n",
    "plt.title(\"Thresholded calls without obvious anomalies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# fitting the gpd. We need a small lr to not hit singularities\n",
    "# We bypass fitting xi here, instead using the xi found from fitting the GEV above. \n",
    "# Theory suggests that it should be close to the optimal value. \n",
    "# We could also profile around it or try full gradient, of course. The latter is brittle\n",
    "\n",
    "\n",
    "xi_gev = daily_max_gev.concentration.numpy()\n",
    "print(f\"Using xi={xi_gev}\")\n",
    "\n",
    "gpd = get_gpd(xi=xi_gev, sigma=1, trainable_xi=False)\n",
    "loss = fit_dist(cleaned_thresholded_calls, gpd, lr=5e-6, num_steps=100, return_animation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# now the qqplot and the stddev\n",
    "\n",
    "std = mle_std_deviations(cleaned_thresholded_calls, gpd)\n",
    "\n",
    "fitted_coeff = tf.convert_to_tensor(gpd.trainable_variables)\n",
    "coeff_upper = fitted_coeff + std\n",
    "coeff_lower = fitted_coeff - std\n",
    "\n",
    "gpd_upper = get_gpd(gpd.concentration.numpy(), *coeff_upper.numpy())\n",
    "gpd_lower = get_gpd(gpd.concentration.numpy(), *coeff_lower.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "qqplot(cleaned_thresholded_calls, gpd)\n",
    "qqplot(cleaned_thresholded_calls, gpd_upper)\n",
    "qqplot(cleaned_thresholded_calls, gpd_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# finding the threshold corresponding to probability of 0.01 of the non-conditioned tail\n",
    "# For that, we rescale with our estimate of N_u\n",
    "\n",
    "N_u = len(cleaned_thresholded_calls)/len(thresholded_n_calls)\n",
    "percentile = 1- N_u*0.01\n",
    "\n",
    "q = u + gpd.quantile(percentile).numpy()\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# and the anomalies lying above it. We find thesame ones\n",
    "\n",
    "n_calls = taxi_df_normalized[\"n_calls\"]\n",
    "taxi_df_normalized[taxi_df_normalized[\"n_calls\"] > q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "We found new candidates for anomalies (or rare events). The 10.01.2015 was the day following Charlie Hebdo related terrorist attacks, there was a large march in Paris. Maybe there was additional movement across New York's large Jewish community. See e.g. [this article](https://www.vogue.com/article/jesuischarlie-new-york-demonstration-charlie-hebdo)\n",
    "\n",
    "We could not find events that could have caused the large numbers of calls on the 18/10/2014 and the 22/11/2014.\n",
    "\n",
    "We also now have a probabilistic model for the tail of n_calls/30 minutes which might be useful for planning taxi availabilities on a more granular level than just per-day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adjusting to seasonality and trends\n",
    "\n",
    "So far, we have completely ignored the time-series aspect of our data set. When using EVT for time series, as will often be the case in practice, seasonality, trends and so on need to be taken into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have already seen a treatment of these topics for time series forecasting. Without going into details, we want to mention that the time-dependency can be to some extent taken into account in EVT by allowing for time-dependent parameters $\\xi(t), \\mu(t), \\sigma(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There exist multiple strategies for finding these time dependent functions from data, the most straightforward one being MLE-fitting with sliding windows over a sample. One could also easily include known modulations into the MLE fitting, e.g. something like \n",
    "\n",
    "$$\n",
    "\\mu(t) : = \\mu_0 \\sin(t)\n",
    "$$\n",
    "\n",
    "might do the job if one knows that the underlying mean vary with $\\sin(t)$. Then, one only needs to fit $\\mu_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion about EVT for anomaly detection\n",
    "\n",
    "Unfortunately, there are some incorrect claims about applications of EVT in the AD literature. The claims often involve an incorrect analysis of EVT for multivariate and multimodal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the EVT theorems apply to univariate distributions. They also ignore multimodality as only tail-behaviour plays a role for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of EVT from one dimension cannot be directly transferred to higher dimensions, even for Gaussians. The cdf of the Mahalanobis radius is simply not dimension independent, see [here](https://upload.wikimedia.org/wikipedia/commons/a/a2/Cumulative_function_n_dimensional_Gaussians_12.2013.pdf) for an exact expression for it. The attempt to do that leads to a bad fit and is sometimes called _failure of classical EVT_. Similar  approaches and resulting claims have been tried on Gaussian mixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EVT for outlier scores\n",
    "\n",
    "Th NYC taxi data is very simple, we could apply EVT to it directly. For multidimensional data these techniques don't work out of the box. However, as mentioned in the beginning, virtually all AD algorithms will produce a 1-dimensional score which can then be given a probabilistic meaning through EVT. We will explore this approach in the last exercise of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we have omitted\n",
    "\n",
    "There are many ways to extend the ideas presented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PoT method can be adapted to work on streams in a memory efficient way, by automatically stripping off obvious anomalies and adjusting the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how MLE with gradient descent is brittle and subject to divergences. There is a lot of literature containing bags of tricks for finding the MLE estimators for GEV and GPT distributions in a smarter, more robust way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also give up on MLE and use _goodness-of-fit_ objectives to minimize the difference with the empirical cdf given by the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, there is a large body of literature on EVT, although more in the engineering/math directions than for AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 8\n",
    "\n",
    "Using the anomaly scores from a data set and algorithm from yesterday (you can choose your favorite), perform an EVT analysis along the lines of what was done above. What are your conclusions? In which situations can such an analysis be useful in practical situations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution of exercise 8: \n",
    "Left to the reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention, this concludes the A.D. training. </div>\n",
    "<div class=\"md-slide title\">We will be happy to see you in another Transferlab training soon!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "rise": {
   "footer": "<img src='aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
