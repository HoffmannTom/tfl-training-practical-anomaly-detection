{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087253b8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The output of this cell should be committed to the repo such that the styles are applied after\n",
    "# opening the notebook\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "with open(\"rise.css\", \"r\") as f:\n",
    "    styles = f.read()\n",
    "HTML(f'<style>{styles}</style>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db38bf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from tfl_training_anomaly_detection.exercise_tools import evaluate, get_kdd_data, get_house_prices_data, create_distributions, contamination, \\\n",
    "perform_rkde_experiment, get_mnist_data\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from tfl_training_anomaly_detection.vae import VAE, build_decoder_mnist, build_encoder_minst, build_contaminated_minst\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2f648",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_first_slide.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Anomaly Detection</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85606a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Density Estimation\n",
    "**Idea:** Estimate the density of $F_0$. Areas of low density are anomalous.\n",
    "- Often $p$ is too small to estimate complete mixture model\n",
    "- Takes into account that $F_1$ might not be well-defined\n",
    "- Estimation procedure needs to be robust against contamination if no clean training data is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad028f22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel Density Estimation\n",
    "- Non-parametric method\n",
    "- Can represent almost arbitrarily shaped densities\n",
    "- Each training point \"spreads\" a fraction of the probability mass as specified by the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f8585",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "---\n",
    "<img src=\"_static/images/kernels.png\" style=\"margin:0px 100px\" width=\"600\" align=\"right\">\n",
    "\n",
    "**Definition:**\n",
    "- $K: \\mathbb{R} \\to \\mathbb{R}$ kernel function\n",
    "    - $K(r) \\geq 0$ for all $r\\in \\mathbb{R}$\n",
    "\t- $\\int_{-\\infty}^{\\infty} K(r) dr = 1$\n",
    "- $h > 0$ bandwidth\n",
    "- Bandwidth is the most crucial parameter\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847bb69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Definition:**\n",
    "Let $D = \\{x_1,\\ldots,x_N\\}\\subset \\mathbb{R}^p$. The KDE with kernel $K$ and bandwidth $h$ is\n",
    "$KDE_h(x, D) = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{h^p}K\\left(\\frac{|x-x_i|}{h}\\right)$\n",
    "\n",
    "---\n",
    "<table style=\"width:60%\">\n",
    "    <tr>\n",
    "        <td width=\"20%\" style=\"background-color:#FFFFFF;\"><img src=\"_static/images/bandwidth.png\" width=\"100\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/1dkde.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\" colspan=\"2\"><center>Effect of bandwidth and kernel</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d0f3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "Play with the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d29d49",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "dists = create_distributions(dim=2, dim_irrelevant=0)\n",
    "\n",
    "sample_train = dists['Double Blob'].sample(500)\n",
    "X_train = sample_train[-1]\n",
    "y_train = [0]*len(X_train)\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c = 'blue', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639512ea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def fit_kde(kernel, bandwidth, X_train):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth)\n",
    "    kde.fit(X_train)\n",
    "    return kde\n",
    "\n",
    "def visualize_kde(kde, bandwidth, X_test, y_test):\n",
    "    fig, axis = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    lin = np.linspace(-10, 10, 50)\n",
    "    grid_points = list(it.product(lin, lin))\n",
    "    ys, xs = np.meshgrid(lin, lin)\n",
    "    # The score function of sklearn returns log-densities\n",
    "    scores = np.exp(kde.score_samples(grid_points)).reshape(50, 50)\n",
    "    colormesh = axis.contourf(xs, ys, scores)\n",
    "    fig.colorbar(colormesh)\n",
    "    axis.set_title('Density Conturs (Bandwidth={})'.format(bandwidth))\n",
    "    axis.set_aspect('equal')\n",
    "    color = ['blue' if i ==0 else 'red' for i in y_test]\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa638a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Choose KDE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5ef25",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ker = None\n",
    "bdw = None\n",
    "@interact(\n",
    "    kernel=['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'],\n",
    "    bandwidth=(.1, 10.)\n",
    ")\n",
    "def set_kde_params(kernel, bandwidth):\n",
    "    global ker, bdw\n",
    "\n",
    "    ker = kernel\n",
    "    bdw = bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c4c5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kde = fit_kde(ker, bdw, X_train)\n",
    "visualize_kde(kde, bdw, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2d6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bandwidth Selection\n",
    "The bandwidth is the most important parameter of a KDE model. A wrongly adjusted value will lead to over- or\n",
    "under-smoothing of the density curve.\n",
    "\n",
    "A common method to select a bandwidth is maximum log-likelihood cross validation.\n",
    "$$h_{\\textrm{llcv}} = \\arg\\max_{h}\\frac{1}{k}\\sum_{i=1}^k\\sum_{y\\in D_i}\\log\\left(\\frac{k}{N(k-1)}\\sum_{x\\in D_{-i}}K_h(x, y)\\right)$$\n",
    "where $D_{-i}$ is the data without the $i$th cross validation fold $D_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4068e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c87f47",
   "metadata": {},
   "source": [
    "ex no.1: Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238d830",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate example\n",
    "dists = create_distributions(dim=2)\n",
    "\n",
    "distribution_with_anomalies = contamination(\n",
    "    nominal=dists['Sinusoidal'],\n",
    "    anomaly=dists['Blob'],\n",
    "    p=0.05\n",
    ")\n",
    "\n",
    "# Train data\n",
    "sample_train = dists['Sinusoidal'].sample(500)\n",
    "X_train = sample_train[-1].numpy()\n",
    "\n",
    "# Test data\n",
    "sample_test = distribution_with_anomalies.sample(500)\n",
    "X_test = sample_test[-1].numpy()\n",
    "y_test = sample_test[0].numpy()\n",
    "\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "handels, _ = scatter.legend_elements()\n",
    "plt.legend(handels, ['Nominal', 'Anomaly'])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095bdb2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# TODO: Define the search space for the kernel and the bandwidth\n",
    "####################################################################################################################\n",
    "param_space = {\n",
    "    'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'], # Add available kernels\n",
    "    'bandwidth': np.linspace(0.1, 10, 100), # Define Search space for bandwidth parameter\n",
    "}\n",
    "\n",
    "def hyperopt_by_score(X_train, param_space, cv=5):\n",
    "    kde = KernelDensity()\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=kde,\n",
    "        param_distributions=param_space,\n",
    "        n_iter=100,\n",
    "        cv=cv,\n",
    "        scoring=None # use estimators internal scoring function, i.e. the log-probability of the validation set for KDE\n",
    "    )\n",
    "\n",
    "    search.fit(X_train)\n",
    "    return search.best_params_, search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed34cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Run the code below to perform hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01513b81",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "params, kde = hyperopt_by_score(X_train, param_space)\n",
    "\n",
    "print('Best parameters:')\n",
    "for key in params:\n",
    "    print('{}: {}'.format(key, params[key]))\n",
    "\n",
    "test_scores = -kde.score_samples(X_test)\n",
    "test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "\n",
    "curves = evaluate(y_test, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_kde(kde, params['bandwidth'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d598cda",
   "metadata": {},
   "source": [
    "### Exercise: Isolate anomalies in house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b4783",
   "metadata": {},
   "source": [
    "You are a company resposible to estimate house prices around Ames, Iowa, specifically around college area. But there is a problem: houses from a nearby area, 'Veenker', are often included in your dataset. You want to build an anomaly detection algorithm that filters one by one every point that comes from the wrong neighborhood. You have been able to isolate an X_train dataset which, you are sure, contains only houses from College area. Following the previous example, test your ability to isolate anomalies in new incoming data (X_test) with KDE.\n",
    "\n",
    "Advanced exercise:\n",
    "What happens if the contamination comes from other areas? You can choose among the following names:\n",
    "\n",
    "OldTown, Veenker, Edwards, MeadowV, Somerst, NPkVill, BrDale, Gilbert, NridgHt, Sawyer, Blmngtn, Blueste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_test = get_house_prices_data(neighborhood = 'CollgCr', anomaly_neighborhood='Veenker')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total data\n",
    "train_test_data = X_train.append(X_test, ignore_index=True)\n",
    "y_total = [0] * len(X_train) + y_test\n",
    "\n",
    "fig = px.scatter_3d(train_test_data, x='LotArea', y='OverallCond', z='SalePrice', color=y_total)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc80809",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336ebbd",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# When data are highly in-homogeneous, like in this case, it is often beneficial \n",
    "# to rescale them before applying any anomaly detection or clustering technique.\n",
    "scaler = MinMaxScaler()\n",
    "X_train_rescaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159cf66",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'], # Add available kernels\n",
    "    'bandwidth': np.linspace(0.1, 10, 100), # Define Search space for bandwidth parameter\n",
    "}\n",
    "params, kde = hyperopt_by_score(X_train_rescaled, param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6217804",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "print('Best parameters:')\n",
    "for key in params:\n",
    "    print('{}: {}'.format(key, params[key]))\n",
    "\n",
    "X_test_rescaled = scaler.transform(X_test)\n",
    "test_scores = -kde.score_samples(X_test_rescaled)\n",
    "test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "curves = evaluate(y_test, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2238bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Curse of Dimensionality\n",
    "The flexibility of KDE comes at a price. The dependency on the dimensionality of the data is quite unfavorable.\n",
    "\n",
    "---\n",
    "*Theorem* [Stone, 1982]\n",
    "Any estimator that is consistent$^*$ with the class of all $k$-fold differentiable pdfs over $\\mathbb{R}^d$ has a\n",
    "convergence rate of at most\n",
    "\n",
    "$$\n",
    "\\frac{1}{n^{\\frac{k}{2k+d}}}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "$^*$Consistency = for all pdfs $p$ in the class: $\\lim_{n\\to\\infty}|KDE_h(x, D) - p(x)|_\\infty = 0$ with probability $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65268b84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "- The very slow convergence in high dimensions does not necessary mean that we will see bad results in high dimensional anomaly detection with KDE.\n",
    "- Especially if the anomalies are very outlying.\n",
    "- However, in cases where contours of the nominal distribution are non-convex we can run into problems.\n",
    "\n",
    "We take a look at a higher dimensional version of out previous data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8b1a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dists = create_distributions(dim=3)\n",
    "\n",
    "distribution_with_anomalies = contamination(\n",
    "    nominal=dists['Sinusoidal'],\n",
    "    anomaly=dists['Blob'],\n",
    "    p=0.01\n",
    ")\n",
    "\n",
    "sample = distribution_with_anomalies.sample(500)\n",
    "\n",
    "y = sample[0]\n",
    "X = sample[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7d48b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Fit KDE on high dimensional examples \n",
    "rocs = []\n",
    "auprs = []\n",
    "bandwidths = []\n",
    "\n",
    "param_space = {\n",
    "        'kernel': ['gaussian'],\n",
    "        'bandwidth': np.linspace(0.1, 100, 1000), # Define Search space for bandwidth parameter\n",
    "    }\n",
    "\n",
    "kdes = {}\n",
    "dims = np.arange(2,16)\n",
    "for d in tqdm(dims):\n",
    "    # Generate d dimensional distributions\n",
    "    dists = create_distributions(dim=d)\n",
    "\n",
    "    distribution_with_anomalies = contamination(\n",
    "        nominal=dists['Sinusoidal'],\n",
    "        anomaly=dists['Blob'],\n",
    "        p=0\n",
    "    )\n",
    "\n",
    "    # Train on clean data\n",
    "    sample_train = dists['Sinusoidal'].sample(500)\n",
    "    X_train = sample_train[-1].numpy()\n",
    "    # Test data\n",
    "    sample_test = distribution_with_anomalies.sample(500)\n",
    "    X_test = sample_test[-1].numpy()\n",
    "    y_test = sample_test[0].numpy()\n",
    "\n",
    "    # Optimize bandwidth\n",
    "    params, kde = hyperopt_by_score(X_train, param_space)\n",
    "    kdes[d] = (params, kde)\n",
    "    \n",
    "    bandwidths.append(params['bandwidth'])\n",
    "\n",
    "    test_scores = -kde.score_samples(X_test)\n",
    "    test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc679493",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot cross section of pdf \n",
    "fig, axes = plt.subplots(nrows=2, ncols=7, figsize=(15, 5))\n",
    "for d, axis in tqdm(list(zip(kdes, axes.flatten()))):\n",
    "    \n",
    "    params, kde = kdes[d]\n",
    "\n",
    "    lin = np.linspace(-10, 10, 50)\n",
    "    grid_points = list(it.product(*([[0]]*(d-2)), lin, lin))\n",
    "    ys, xs = np.meshgrid(lin, lin)\n",
    "    # The score function of sklearn returns log-densities\n",
    "    scores = np.exp(kde.score_samples(grid_points)).reshape(50, 50)\n",
    "    colormesh = axis.contourf(xs, ys, scores)\n",
    "    axis.set_title(\"Dim = {}\".format(d))\n",
    "    axis.set_aspect('equal')\n",
    "    \n",
    "\n",
    "# Plot evaluation\n",
    "print('Crossection of the KDE at (0,...,0, x, y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020500ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robustness\n",
    "Another drawback of KDE in the context of anomaly detection is that it is not robust against contamination of the data\n",
    "\n",
    "---\n",
    "**Definition**\n",
    "The *breakdown point* of an estimator is the smallest fraction of observations that need to be changed so that we can\n",
    "move the estimate arbitrarily far away from the true value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf4c13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**: The sample mean has a breakdown point of $0$. Indeed, for a sample of $x_1,\\ldots, x_n$ we only need to\n",
    "change a single value in order to move the sample mean in any way we want. That means that the breakdown point is\n",
    "smaller than $\\frac{1}{n}$ for every $n\\in\\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efce06e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robust Statistics\n",
    "There are robust replacements for the sample mean:\n",
    "- Median of means: Split the dataset into $S$ equally sized subsets $X_1,\\ldots, X_S$ and compute\n",
    "$\\mathrm{median}(\\overline{X_1},\\ldots, \\overline{X_S})$\n",
    "- M-estimation: The mean in a normed vector space is the value that minimizes the squared distances\n",
    "<center>\n",
    "$\\overline{X} = \\min_{y}\\sum_{x\\in X}|x-y|^2$\n",
    "</center>\n",
    "M-estimation replaces the quadratic loss with a more robust loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5655c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Huber loss\n",
    "Switch from quadratic to linear loss at prescribed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6861d26",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def huber(error, threshold):\n",
    "    test = (np.abs(error) <= threshold)\n",
    "    return (test * (error**2)/2) + ((1-test)*threshold*(np.abs(error) - threshold/2))\n",
    "\n",
    "x = np.linspace(-5, 5)\n",
    "y = huber(x, 1)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set_title(\"Huber Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017109f",
   "metadata": {},
   "source": [
    "### Hampel loss\n",
    "More complex loss function. Depends on 3 parameters 0 < a < b< r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f95fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def single_point_hampel(error, a, b, r):\n",
    "    if abs(error) <= a:\n",
    "        return error**2/2\n",
    "    elif a < abs(error) <= b:\n",
    "        return (1/2 *a**2 + a* (abs(error)-a))\n",
    "    elif  b < abs(error) <= r:\n",
    "        return a * (2*b-a+(abs(error)-b) * (1+ (r-abs(error))/(r-b)))/2\n",
    "    else:\n",
    "        return a*(b-a+r)/2\n",
    "\n",
    "hampel = np.vectorize(single_point_hampel)\n",
    "\n",
    "x = np.linspace(-10.1, 10.1)\n",
    "y = hampel(x, a=1.5, b=3.5, r=8)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set_title(\"Hampel Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed645c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KDE is a Mean\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel as scalar product:**\n",
    "<img src=\"_static/images/kernels.png\" align=\"right\" width=\"400\" style=\"margin: 150px 75px\">\n",
    "\n",
    "- Let $K$ be a radial monotonic$^\\ast$ kernel over $\\mathbb{R}^n$.\n",
    "- For $x\\in\\mathbb{R}^n$ let $\\phi_x = K(\\cdot, x)$.\n",
    "- Vector space over the linear span of $\\{\\phi_x \\mid x\\in\\mathbb{R}^n\\}$:\n",
    "    - Pointwise addition and scalar multiplication.\n",
    "- Define the scalar product $\\langle \\phi_x, \\phi_y\\rangle = K(x,y)$.\n",
    "- Advantage: Scalar product is computable\n",
    "- Call this the reproducing kernel Hilbert space (RKHS) of $K$.\n",
    "- $\\mathrm{KDE}_h(\\cdot, D) = \\frac{1}{N}\\sum_{i=1}^N K_h(\\cdot, x_i) = \\frac{1}{N}\\sum_{i=1}^N\\phi_{x_i}$\n",
    "    - where $K_h(x,y) = \\frac{1}{h}K\\left(\\frac{|x-y|}{h}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "$^*$All kernels that we have seen are radial and monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e6487",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "We compare the performance of different approaches to recover the nominal distribution under contamination.\n",
    "Here, we use code by [Humbert et al.](https://github.com/lminvielle/mom-kde) to replicate\n",
    "the results in the referenced paper on median-of-mean KDE. More details on rKDE can instead be found in this paper by [Kim and Scott.](https://arxiv.org/abs/1107.3133#:~:text=We%20propose%20a%20method%20for,ideas%20from%20classical%20M%2Destimation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58243128",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "#   Parameters\n",
    "# =======================================================\n",
    "algos = [\n",
    "    'kde',\n",
    "    'mom-kde', # Median-of-Means\n",
    "    'rkde-huber', # robust KDE with huber loss\n",
    "    'rkde-hampel', # robust KDE with hampel loss\n",
    "]\n",
    "\n",
    "dataset = 'house-prices'\n",
    "dataset_options = {'neighborhood': 'CollgCr', 'anomaly_neighborhood': 'Edwards'}\n",
    "\n",
    "outlierprop_range = [0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "kernel = 'gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da628e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "auc_scores = perform_rkde_experiment(\n",
    "    algos,\n",
    "    dataset,\n",
    "    dataset_options,\n",
    "    outlierprop_range,\n",
    "    kernel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ed959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "for algo, algo_data in auc_scores.groupby('algo'):\n",
    "    x = algo_data.groupby('outlier_prop').mean().index\n",
    "    y = algo_data.groupby('outlier_prop').mean()['auc_anomaly']\n",
    "    ax.plot(x, y, 'o-', label=algo)\n",
    "plt.legend()\n",
    "plt.xlabel('outlier_prop')\n",
    "plt.ylabel('auc_score')\n",
    "plt.title('Comparison of rKDE against contamination')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd659a73",
   "metadata": {},
   "source": [
    "Try using different neighborhoods for contamination. Which robust KDE algorithm performs better overall? Choose among the following options:\n",
    "\n",
    "OldTown, Veenker, Edwards, MeadowV, Somerst, NPkVill, BrDale, Gilbert, NridgHt, Sawyer, Blmngtn, Blueste\n",
    "\n",
    "You can also change the kernel type: gaussian, tophat, epechenikov, exponential, linear or cosine, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76312d0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary\n",
    "- Kernel density estimation is a non-parametric method to estimate a pdf from a sample.\n",
    "- Bandwidth is the most important parameter.\n",
    "- Converges to the true pdf if $n\\to\\infty$.\n",
    "    - Convergence exponentially depends on the dimension.\n",
    "- KDE is sensitive to contamination:\n",
    "    - In a contaminated setting one can employ methods from robust statistics to obtain robust estimates.\n",
    "    \n",
    "## Implementations\n",
    "- Sklearn: [sklearn.neighbors.KernelDensity](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)\n",
    "- Statsmodels: [statsmodels.nonparametric.kernel_density.KDEMultivariate](https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html)\n",
    "- FastKDE: [link](https://pypi.org/project/fastkde/), offers automatic bandwidth and kernel selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e46caa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Isolation\n",
    "**Idea:** An anomaly should allow  \"simple\" descriptions that distinguish it from the rest of the data.\n",
    "\n",
    "- Descriptions: Conjunction of single attribute tests, i.e.\n",
    "  $X_i \\leq c$ or $X_i > c$.\n",
    "- Example: $X_1 \\leq 1.2 \\text{ and } X_5 > -3.4 \\text{ and }\tX_7 \\leq 5.6$.\n",
    "- Complexity of description: Number of conjunctions.\n",
    "\n",
    "Moreover, we assume that a short random descriptions will have a significantly larger chance of isolating an anomaly\n",
    "than isolating any nominal point.\n",
    "\n",
    "- Choose random isolating descriptions and compute anomaly score from average complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce4a9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Isolation Tree\n",
    "Isolation Forest (iForest) implements this idea by generating an ensemble of random decision trees.\n",
    "Each tree is built as follows:\n",
    "\n",
    "---\n",
    "**Input:** Data set (subsample) $X$, maximal height $h$\n",
    "- Randomly choose feature $i$ and split value $s$ (in range of data)\n",
    "- Recursively build subtrees on $X_L = \\{x\\in X\\mid x_i \\leq s\\}$ and $X_R = X\\setminus X_L$\n",
    "- Stop if remaining data set $ \\leq 1$ or maximal height reached\n",
    "- Store test $x_i\\leq s$ for inner nodes and $|X|$ for leaf nodes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dabe54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualization\n",
    "<center>\n",
    "<img src=\"_static/images/partition.png\" align=\"center\" width=\"400\">\n",
    "    Isolation Tree as Partition Diagram\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc2c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Isolation Depth\n",
    "\n",
    "---\n",
    "**Input:** Observation $x$\n",
    "- ${\\ell} = $ length of path from root to leaf according to tests\n",
    "- ${n} = $ size of remaining data set in leaf node\n",
    "- ${c(n)} =$ expected length of a path in a BST with $n$ nodes $={O}(\\log n)$\n",
    "- ${h(x)} = \\ell + c(n)$\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/blob.png\" width=\"300\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/tree.png\"  width=\"300\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" style=\"background-color:#FFFFFF;\"><center>Isolation Depth of Outlier (red) and nominal (blue)</center></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659e094",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Isolation Forest\n",
    "- Train $k$ isolation trees on subsamples of size $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423122d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/inrad.png\" width=\"400\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/outrad.png\"  width=\"400\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\"><center>Isolation depth of nominal point (left) and outlier (right)</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a372e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Variants of Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9985",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Variant: Random Robust Cut Forest\n",
    "**New Rule to Choose Split Test:**\n",
    "- $\\ell_i$: length of the $i$th component of the bounding box around current data set\n",
    "- Choose dimension $i$ with probability $\\frac{\\ell_i}{\\sum_j \\ell_j}$\n",
    "- More robust against \"noise dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a2568",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/partition2.png\" align=\"center\" width=\"400\" caption=\"Comparisson IForest and RRCF\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f67a8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Variant: Extended Isolation Forest\n",
    "**New split criterion:**\n",
    "- Uniformly choose a normal and an orthogonal hyperplane through the data\n",
    "- Removes a bias that was empirically observed when plotting the outlier score of iForest on low dimensional data sets\n",
    "\n",
    "<center>\n",
    "<img src=\"_static/images/extended.png\" align=\"center\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144c10d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise: Network Security\n",
    "\n",
    "In the final exercise of today you will have to develop an anomaly detection system for network traffic.\n",
    "\n",
    "## Briefing\n",
    "A large e-commerce company __A__ is experiencing downtime due to attacks on their infrastructure.\n",
    "You were instructed to develop a system that can detect malicious connections to the infrastructure.\n",
    "It is planned that suspicious clients will be banned.\n",
    "\n",
    "Another data science team already prepared the connection data of the last year for you. They also separated a test set and manually identified and labeled attacks in that data.\n",
    "\n",
    "## The Data\n",
    "We will work on a version of the classic KDD99 data set.\n",
    "\n",
    "### Kddcup 99 Data Set\n",
    "----------------------------\n",
    "The KDD Cup '99 dataset was created by processing the tcp dump portions\n",
    "of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n",
    "created by MIT Lincoln Lab [1]. The artificial data (described on the `dataset's\n",
    "homepage <https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was\n",
    "generated using a closed network and hand-injected attacks to produce a\n",
    "large number of different types of attack with normal activity in the\n",
    "background.\n",
    "\n",
    "    =========================   ====================================================\n",
    "    Samples total               976158\n",
    "    Dimensionality              41\n",
    "    Features                    string (str), discrete (int), continuous (float)\n",
    "    Targets                     str, 'normal.' or name of the anomaly type\n",
    "    Proportion of Anomalies     1%\n",
    "    =========================   ====================================================\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "## Task\n",
    "You will have to develop the system on your own. In particular, you will have to\n",
    "- Explore the data.\n",
    "- Choose an algorithm.\n",
    "- Find a good detection threshold.\n",
    "- Evaluate and summarize your results.\n",
    "- Estimate how much __A__ could save through the use of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e31b17",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_test = get_kdd_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47f0c1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044d551",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Add your exploration code\n",
    "#\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4d539",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# get description\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1483fbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# get better description\n",
    "X_train.drop(columns=[1,2,3]).astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2ddd4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "print(\"Number of NaNs: {}\".format(X_train.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c76dc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Add your preperation code here\n",
    "#\n",
    "\n",
    "# Encode string features\n",
    "binarizer = LabelBinarizer()\n",
    "one_hots = None\n",
    "one_hots_test = None\n",
    "for i in [1, 2, 3]:\n",
    "    binarizer.fit(X_train[[i]].astype(str))\n",
    "    if one_hots is None:\n",
    "        one_hots = binarizer.transform(X_train[[i]].astype(str))\n",
    "        one_hots_test = binarizer.transform(X_test[[i]].astype(str))\n",
    "    else:\n",
    "        one_hots = np.concatenate([one_hots, binarizer.transform(X_train[[i]].astype(str))], axis=1)\n",
    "        one_hots_test = np.concatenate([one_hots_test, binarizer.transform(X_test[[i]].astype(str))], axis=1)\n",
    "\n",
    "X_train.drop(columns=[1,2,3], inplace=True)\n",
    "X_train_onehot = pd.DataFrame(np.concatenate([X_train.values, one_hots], axis=1))\n",
    "\n",
    "X_test.drop(columns=[1,2,3], inplace=True)\n",
    "X_test_onehot = pd.DataFrame(np.concatenate([X_test.values, one_hots_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d363320",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Encode y\n",
    "y_test_bin = np.where(y_test == b'normal.', 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cf969",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Remove suspicious data\n",
    "# This step is not strictly neccessary but can improve performance\n",
    "suspicious = X_train_onehot.apply(lambda col: (col - col.mean()).abs() > 4 * col.std() if col.std() > 1 else False)\n",
    "suspicious = suspicious.any(axis=1)# 4 sigma rule\n",
    "print('filtering {} suspicious data points'.format(suspicious.sum()))\n",
    "X_train_clean = X_train_onehot[~suspicious]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f1dfb",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Isolation Forest empirically shows very good performance up to relatively high dimensions\n",
    "- It is relatively robust against contamination\n",
    "- Usually little need for hyperparameter tuning\n",
    "\n",
    "## Implementations\n",
    "- Sklearn: [sklearn.ensemble.IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)\n",
    "- Extended Isolation Forest: [variant](https://github.com/sahandha/eif)\n",
    "- Random Robust Cut Forest: [variant](https://github.com/kLabUM/rrcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80805d83",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Choose Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d462c9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement proper model selection\n",
    "iforest = IsolationForest()\n",
    "iforest.fit(X_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877e13d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# find best threshold\n",
    "X_test_onehot, X_val_onehot, y_test_bin, y_val_bin = train_test_split(X_test_onehot, y_test_bin, test_size=.5)\n",
    "y_score = -iforest.score_samples(X_val_onehot).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338159f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Evaluate Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920afcf1",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Insert evaluation code\n",
    "#\n",
    "\n",
    "# calculate scores if any anomaly is present\n",
    "if np.any(y_val_bin == 1):\n",
    "    eval = evaluate(y_val_bin, y_score)\n",
    "    prec, rec, thr = eval['PR']\n",
    "    f1s = 2 * (prec * rec)/(prec + rec)\n",
    "    threshold = thr[np.argmax(f1s)]\n",
    "\n",
    "    y_score = -iforest.score_samples(X_test_onehot).reshape(-1)\n",
    "    y_pred = np.where(y_score < threshold, 0, 1)\n",
    "\n",
    "    print('Precision: {}'.format(metrics.precision_score(y_test_bin, y_pred)))\n",
    "    print('Recall: {}'.format(metrics.recall_score(y_test_bin, y_pred)))\n",
    "    print('F1: {}'.format(metrics.f1_score(y_test_bin, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287222a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Reconstruction Error\n",
    "**Idea:** Embed the data into low dimensional space and reconstruct it again.\n",
    "\t\t\tGood embedding of nominal data $\\Rightarrow$ high reconstruction error indicates anomaly.\n",
    "\n",
    "**Autoencoder:**\n",
    "- Parametric family of encoders: $f_\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{\\text{low}}$\n",
    "- Parametric family of decoders: $g_\\theta: \\mathbb{R}^{\\text{low}} \\to \\mathbb{R}^{d}$\n",
    "- Reconstruction error of $(f_\\phi, g_\\theta)$ on $x$: $|x - g_\\theta(f_\\phi(x))|$\n",
    "- Given data set $D$, find $\\phi,\\theta$ that minimize $\\sum_{x\\in D} L(|x- g_\\theta(f_\\phi(x))|) $\n",
    "  for some loss function $L$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5094e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualization\n",
    "<center>\n",
    "<img src=\"_static/images/autoencoder.png\" align=\"center\" width=\"400\">\n",
    "    Autoencoder Schema\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a43e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "Neural networks are very well suited for finding low dimensional representations of data. Hence they are a popular choice for the encoder and the decoder.\n",
    "\n",
    "---\n",
    "**Artificial Neuron with $N$ inputs:** $y = \\sigma\\left(\\sum_i^N w_i X_i + b\\right)$\n",
    "\n",
    "- $\\sigma$: nonlinear activation-function (applied component wise).\n",
    "- $b$ bias\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658847c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table style=\"background-color:#FFFFFF;\">\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/neuron.png\" width=\"200\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/activations.png\"  width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" style=\"background-color:#FFFFFF;\"><center>Isolation depth of nominal point and anomaly</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09243719",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks combine many artificial neurons into a complex network. These networks are usually organized in layers\n",
    "where the result of each layer is the input for the next layer. Some commonly used layers are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376f048",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/nn_layers.png\" align=\"center\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8794297",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variational Autoencoders\n",
    "An important extension of autoencoders that relates the idea to density estimation.\n",
    "More precisely, we define a generative model for our data using latent variables and combine the maximum likelihood\n",
    "estimation of the parameters with a simultaneous posterior estimation of the latents through amortized stochastic\n",
    "variational inference. We use a decoder network to transform the latent variables into the data distribution, and an\n",
    "encoder network to compute the posterior distribution of the latents given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5efeac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "**Definition:**\n",
    "The model uses an observed variable $X$ (the data) and a latent variable $Z$ (the defining features of $X$). We assume\n",
    "both $P(Z)$ and $P(X\\mid Z)$ to be normally distributed. More precisely\n",
    "\n",
    "- $P(Z) = \\mathcal{N}(0, I)$\n",
    "- $P(X\\mid Z) = \\mathcal{N}(\\mu_\\phi(Z), I)$\n",
    "\n",
    "where $\\mu_\\phi$ is a neural network parametrized with $\\phi$.\n",
    "We use variational inference to perform posterior inference on $Z$ given $X$. We assume that the distribution $P(Z\\mid X)$\n",
    "to be relatively well approximated by a Gaussian and use the posterior approximation:\n",
    "- $q(X\\mid Z) = \\mathcal{N}(\\mu_\\psi(X), \\sigma_\\psi(X))$\n",
    "\n",
    "$\\mu_\\psi$ and $\\sigma_\\psi$ are neural networks parameterized with $\\psi$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d013d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/vae_schema.png\" align=\"center\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81661bf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a data set $D$ we minimize the (amortized) Kullback-Leibler divergence between our posterior approximation and the\n",
    "true posterior:\n",
    "\\begin{align*}\n",
    "  D_{KL}(q(z\\mid x),p(z\\mid x)) &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(z \\mid X)}\\right)\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid X)}\\left[\\log\\left(\\frac{q(z \\mid x)}{\\frac{p(x \\mid z)p(z)}{p(x)}}\\right)\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right) + \\log(p(x))\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right)\\right]  + E_{x\\sim X}[\\log(p(x))]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now we can define\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{ELBO}(q(z\\mid x),p(z\\mid x)) &:= E_{x\\sim X}[\\log(p(x))] - D_{KL}(q(z\\mid x),p(z\\mid x)) \\\\\n",
    "        &= -E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Note that we can evaluate the expression inside the expectation of the final RHS of the\n",
    "equation and we can obtain unbiased estmates of the expectation via sampling.\n",
    "Let us further try to understand the ELBO as an optimization objective. On one hand, maximizing the ELBO with respect to the parameters in $q$ is equivalent to\n",
    "minimizing the KL divergence between $p$ and $q$. On the other hand, maximizing the ELBO with\n",
    "respect to the parameters in $p$ can be understood as raising a lower bound for the likelihood of the\n",
    "generative model $p(x)$. Hence, the optimization tries to find an encoder and a decoder pair such that\n",
    "it simultaneously provides a good generative explanation of the data and a good approximation of the posterior\n",
    "distribution of the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131628c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8c5a6",
   "metadata": {},
   "source": [
    "# The MNIST Data Set\n",
    "MNIST is one of the most iconic data sets in the history of machine learning.\n",
    "It contains 70000 samples of $28\\times 28$ grayscale images of handwritten digits.\n",
    "Because of its moderate complexity and good visualizability it is well suited to study the behavior of machine learning\n",
    "algorithms in higher dimensional spaces.\n",
    "\n",
    "While originally created for classification (optical character recognition), we can build an anomaly detection data set\n",
    "by corrupting some of the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28979a57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Pre-processing\n",
    "We first need to obtain the MNIST data set and prepare an anomaly detection set from it.\n",
    "Note that the data set is n row vector format.\n",
    "Therefore, we work with $28\\times 28 = 784$ dimensional data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2508225",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load MNIST Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d8450",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mnist = get_mnist_data()\n",
    "\n",
    "data = mnist['data']\n",
    "print('data.shape: {}'.format(data.shape))\n",
    "target = mnist['target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6bde9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Build contaminated Data Sets\n",
    "We prepared a function that does the job for us.\n",
    "It corrupts a prescribed portion of the data by introducing a rotation, noise or a blackout of some part of the image.\n",
    "\n",
    "First, we need to transform the data into image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3762",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X = data.reshape(-1, 28, 28, 1)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6c089",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Train/Test-Split\n",
    "We will only corrupt the test set, hence we will perform the train-test split beforehand.\n",
    "We separate a relatively small test set so that we can use as much as possible from the data to obtain high quality\n",
    "representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb9c57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "X_train, X_test, target_train, target_test = train_test_split(X, target, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad3d43",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = build_contaminated_minst(X_test)\n",
    "\n",
    "# Visualize contamination\n",
    "anomalies = X_test[y_test != 0]\n",
    "selection = np.random.choice(len(anomalies), 25)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(5, 5))\n",
    "for img, ax in zip(anomalies[selection], axes.flatten()):\n",
    "    ax.imshow(img, 'gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c459a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Autoencoder\n",
    "Let us finally train an autoencoder model. We replicate the model given in the\n",
    "[Keras documentation](https://keras.io/examples/generative/vae/) and apply it in a synthetic outlier detection scenario\n",
    "based on MNIST.\n",
    "\n",
    "in the vae package we provide the implementation of the VAE. Please take a look into the source code to see how\n",
    "the minimization of the KL divergence is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af1c41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90996a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "vae = VAE(decoder=build_decoder_mnist(latent_dim=latent_dim), encoder=build_encoder_minst(latent_dim=latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb89bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect model architecture\n",
    "vae.encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b219e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect model architecture\n",
    "vae.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b43aff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "n_epochs = 30\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=.001))\n",
    "history = vae.fit(X_train, epochs=n_epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1519875",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Inspect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab41fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=10, figsize=10):\n",
    "    for perm in [[0, 1, 2], [1, 2, 0], [2, 1, 0]]:\n",
    "        # display a n*n 2D manifold of digits\n",
    "        digit_size = 28\n",
    "        scale = 1.0\n",
    "        figure = np.zeros((digit_size * n, digit_size * n))\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        grid_x = np.linspace(-scale, scale, n)\n",
    "        grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z_sample = np.array([[xi, yi, 0]])\n",
    "                z_sample[0] = z_sample[0][perm]\n",
    "                x_decoded = vae.decoder.predict(z_sample)\n",
    "                digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "                figure[\n",
    "                    i * digit_size : (i + 1) * digit_size,\n",
    "                    j * digit_size : (j + 1) * digit_size,\n",
    "                ] = digit\n",
    "\n",
    "        plt.figure(figsize=(figsize, figsize))\n",
    "        start_range = digit_size // 2\n",
    "        end_range = n * digit_size + start_range\n",
    "        pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z[{}]\".format(perm[0]))\n",
    "        plt.ylabel(\"z[{}]\".format(perm[1]))\n",
    "        plt.gca().set_title('z[{}] = 0'.format(perm[2]))\n",
    "        plt.imshow(figure, cmap=\"Greys_r\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0f67d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_latent_space(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a5b6f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Principal components\n",
    "pca = PCA()\n",
    "latents = vae.encoder.predict(X_train)[2]\n",
    "pca.fit(latents)\n",
    "\n",
    "kwargs = {'x_{}'.format(i): (-1., 1.) for i in range(latent_dim)}\n",
    "\n",
    "\n",
    "@widgets.interact(**kwargs)\n",
    "def explore_latent_space(**kwargs):\n",
    "    center_img = pca.transform(np.zeros([1,latent_dim]))\n",
    "\n",
    "    latent_rep_pca =  center_img + np.array([[kwargs[key] for key in kwargs]])\n",
    "    latent_rep = pca.inverse_transform(latent_rep_pca)\n",
    "    img = vae.decoder(latent_rep).numpy().reshape(28, 28)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.imshow(img,cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fb82f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latents = vae.encoder.predict(X_train)[2]\n",
    "scatter = px.scatter_3d(x=latents[:, 0], y=latents[:, 1], z=latents[:, 2], color=target_train)\n",
    "\n",
    "scatter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea370a83",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latents = vae.encoder.predict(X_test)[2]\n",
    "scatter = px.scatter_3d(x=latents[:, 0], y=latents[:, 1], z=latents[:, 2], color=y_test)\n",
    "\n",
    "scatter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0a98e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c957f8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "s = np.random.choice(range(len(X_val)), n_samples)\n",
    "s = X_val[s]\n",
    "#s = [X_train_img[i] for i in s]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(10, 2))\n",
    "for img, ax_row in zip(s, axes.T):\n",
    "    x = vae.decoder.predict(vae.encoder.predict(img.reshape(1, 28, 28, 1))[2]).reshape(28, 28)\n",
    "    diff = x - img.reshape(28, 28)\n",
    "    error = (diff * diff).sum()\n",
    "    ax_row[0].axis('off')\n",
    "    ax_row[1].axis('off')\n",
    "    ax_row[0].imshow(img,cmap='gray', vmin=0, vmax=1)\n",
    "    ax_row[1].imshow(x, cmap='gray', vmin=0, vmax=1)\n",
    "    ax_row[1].set_title('E={:.1f}'.format(error))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350edb6c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_test_bin = y_test.copy()\n",
    "y_test_bin[y_test != 0] = 1\n",
    "y_val_bin = y_val.copy()\n",
    "y_val_bin[y_val != 0] = 1\n",
    "# Evaluate\n",
    "reconstruction = vae.decoder.predict(vae.encoder(X_val)[2])\n",
    "rerrors = (reconstruction - X_val).reshape(-1, 28*28)\n",
    "rerrors = (rerrors * rerrors).sum(axis=1)\n",
    "\n",
    "# Let's calculate scores if any anomaly is present\n",
    "if np.any(y_val_bin == 1):\n",
    "    eval = evaluate(y_val_bin.astype(int), rerrors.astype(float))\n",
    "    pr, rec, thr = eval['PR']\n",
    "    f1s = (2 * ((pr * rec)[:-1]/(pr + rec)[:-1]))\n",
    "    threshold = thr[np.argmax(f1s)]\n",
    "    print('Optimal threshold: {}'.format(threshold))\n",
    "\n",
    "    reconstruction = vae.decoder.predict(vae.encoder(X_test)[2])\n",
    "    reconstruction_error = (reconstruction - X_test).reshape(-1, 28*28)\n",
    "    reconstruction_error = (reconstruction_error * reconstruction_error).sum(axis=1)\n",
    "\n",
    "\n",
    "    classification = (reconstruction_error > threshold).astype(int)\n",
    "\n",
    "    print('Precision: {}'.format(metrics.precision_score(y_test_bin, classification)))\n",
    "    print('Recall: {}'.format(metrics.recall_score(y_test_bin, classification)))\n",
    "    print('F1: {}'.format(metrics.f1_score(y_test_bin, classification)))\n",
    "\n",
    "    metrics.confusion_matrix(y_test_bin, classification)\n",
    "else:\n",
    "    reconstruction_error = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5568d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sort Data by Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b304ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if reconstruction_error is not None:\n",
    "    combined = list(zip(X_test, reconstruction_error))\n",
    "    combined.sort(key = lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fd7f3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Show Top Autoencoder Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d7a5c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if reconstruction_error is not None:\n",
    "    n_rows = 10\n",
    "    n_cols = 10\n",
    "    n_samples = n_rows*n_cols\n",
    "\n",
    "    samples = [c[0] for c in combined[-n_samples:]]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(2*n_cols, 2*n_rows))\n",
    "    for img, ax in zip(samples, axes.reshape(-1)):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(img.reshape((28,28)), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd3f88",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Autoencoders are the most prominent reconstruction error based anomaly detection method.\n",
    "- Can provide high quality results on high dimensional data.\n",
    "- Architecture is highly adaptable to the data (fully connected, CNN, attention,...).\n",
    "- Sensitive to contamination.\n",
    "- Variational autoencoder are an important variant the improves the interpretability of the latent space.\n",
    "\n",
    "## Implementations\n",
    "- Keras: see vae.py or [here](https://keras.io/examples/generative/vae/)\n",
    "- Pytorch: [example implementation](https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb)\n",
    "- Pyro (pytorch based probabilistic programming language): [example implementation](https://pyro.ai/examples/vae.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd207a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dab0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
