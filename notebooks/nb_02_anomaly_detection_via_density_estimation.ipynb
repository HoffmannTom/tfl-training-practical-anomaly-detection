{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634d79e",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_anomaly_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596df825",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158112af",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8e783",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2f648",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Density Estimation\n",
    "<img src=\"_static/images/aai_presentation_first_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from tfl_training_anomaly_detection.exercise_tools import evaluate, get_kdd_data, get_house_prices_data, create_distributions, contamination, \\\n",
    "perform_rkde_experiment, get_mnist_data\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from tfl_training_anomaly_detection.vae import VAE, build_decoder_mnist, build_encoder_minst, build_contaminated_minst\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "cb85606a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Density Estimation\n",
    "**Idea:** Estimate the density of $F_0$. Areas of low density are anomalous.\n",
    "- Often $p$ is too small to estimate complete mixture model\n",
    "- Takes into account that $F_1$ might not be well-defined\n",
    "- Estimation procedure needs to be robust against contamination if no clean training data is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad028f22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel Density Estimation\n",
    "- Non-parametric method\n",
    "- Can represent almost arbitrarily shaped densities\n",
    "- Each training point \"spreads\" a fraction of the probability mass as specified by the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f8585",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition**\n",
    "\n",
    "---\n",
    "<img src=\"_static/images/kernels.png\" style=\"margin:0px 100px\" width=\"600\" align=\"right\">\n",
    "\n",
    "**Definition:**\n",
    "- $K: \\mathbb{R} \\to \\mathbb{R}$ kernel function\n",
    "    - $K(r) \\geq 0$ for all $r\\in \\mathbb{R}$\n",
    "\t- $\\int_{-\\infty}^{\\infty} K(r) dr = 1$\n",
    "- $h > 0$ bandwidth\n",
    "- Bandwidth is the most crucial parameter\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847bb69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:**\n",
    "\n",
    "---\n",
    "Let $D = \\{x_1,\\ldots,x_N\\}\\subset \\mathbb{R}^p$. The KDE with kernel $K$ and bandwidth $h$ is\n",
    "$KDE_h(x, D) = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{h^p}K\\left(\\frac{|x-x_i|}{h}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "    <tr>\n",
    "        <td width=\"20%\" style=\"background-color:#FFFFFF;\"><img src=\"_static/images/bandwidth.png\" width=\"100\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/1dkde.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\" colspan=\"2\"><center>Effect of bandwidth and kernel</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d0f3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "Play with the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d29d49",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "dists = create_distributions(dim=2, dim_irrelevant=0)\n",
    "\n",
    "sample_train = dists['Double Blob'].sample(500)\n",
    "X_train = sample_train[-1]\n",
    "y_train = [0]*len(X_train)\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c = 'blue', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639512ea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def fit_kde(kernel: str, bandwidth: float, X_train: np.array) -> KernelDensity:\n",
    "    \"\"\" Fit KDE\n",
    "    \n",
    "    @param kernel: kernel\n",
    "    @param bandwidth: bandwidth\n",
    "    @param x_train: data\n",
    "    \"\"\"\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth)\n",
    "    kde.fit(X_train)\n",
    "    return kde\n",
    "\n",
    "def visualize_kde(kde: KernelDensity, bandwidth: float, X_test: np.array, y_test: np.array) -> None:\n",
    "    \"\"\"Plot KDE\n",
    "    \n",
    "    @param kde: KDE\n",
    "    @param bandwidth: bandwidth\n",
    "    @param X_test: test data\n",
    "    @param y_test: test label\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    lin = np.linspace(-10, 10, 50)\n",
    "    grid_points = list(it.product(lin, lin))\n",
    "    ys, xs = np.meshgrid(lin, lin)\n",
    "    # The score function of sklearn returns log-densities\n",
    "    scores = np.exp(kde.score_samples(grid_points)).reshape(50, 50)\n",
    "    colormesh = axis.contourf(xs, ys, scores)\n",
    "    fig.colorbar(colormesh)\n",
    "    axis.set_title('Density Conturs (Bandwidth={})'.format(bandwidth))\n",
    "    axis.set_aspect('equal')\n",
    "    color = ['blue' if i ==0 else 'red' for i in y_test]\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa638a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Choose KDE Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5ef25",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ker = None\n",
    "bdw = None\n",
    "@interact(\n",
    "    kernel=['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'],\n",
    "    bandwidth=(.1, 10.)\n",
    ")\n",
    "def set_kde_params(kernel: str, bandwidth: float) -> None:\n",
    "    \"\"\"Helper funtion to set widget parameters\n",
    "    \n",
    "    @param kernel: kernel\n",
    "    @param bandwidth: bandwidth\n",
    "    \"\"\"\n",
    "    global ker, bdw\n",
    "\n",
    "    ker = kernel\n",
    "    bdw = bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c4c5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kde = fit_kde(ker, bdw, X_train)\n",
    "visualize_kde(kde, bdw, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2d6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bandwidth Selection\n",
    "The bandwidth is the most important parameter of a KDE model. A wrongly adjusted value will lead to over- or\n",
    "under-smoothing of the density curve.\n",
    "\n",
    "A common method to select a bandwidth is maximum log-likelihood cross validation.\n",
    "$$h_{\\textrm{llcv}} = \\arg\\max_{h}\\frac{1}{k}\\sum_{i=1}^k\\sum_{y\\in D_i}\\log\\left(\\frac{k}{N(k-1)}\\sum_{x\\in D_{-i}}K_h(x, y)\\right)$$\n",
    "where $D_{-i}$ is the data without the $i$th cross validation fold $D_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4068e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c87f47",
   "metadata": {},
   "source": [
    "ex no.1: Noisy sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238d830",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate example\n",
    "dists = create_distributions(dim=2)\n",
    "\n",
    "distribution_with_anomalies = contamination(\n",
    "    nominal=dists['Sinusoidal'],\n",
    "    anomaly=dists['Blob'],\n",
    "    p=0.05\n",
    ")\n",
    "\n",
    "# Train data\n",
    "sample_train = dists['Sinusoidal'].sample(500)\n",
    "X_train = sample_train[-1].numpy()\n",
    "\n",
    "# Test data\n",
    "sample_test = distribution_with_anomalies.sample(500)\n",
    "X_test = sample_test[-1].numpy()\n",
    "y_test = sample_test[0].numpy()\n",
    "\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "handels, _ = scatter.legend_elements()\n",
    "plt.legend(handels, ['Nominal', 'Anomaly'])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d255dc",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## TODO: Define the search space for the kernel and the bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d0c88",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'], # Add available kernels\n",
    "    'bandwidth': np.linspace(0.1, 10, 100), # Define Search space for bandwidth parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095bdb2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def hyperopt_by_score(X_train: np.array, param_space: dict, cv: int=5):\n",
    "    \"\"\"Performs hyperoptimization by score\n",
    "    \n",
    "    @param X_train: data\n",
    "    @param param_space: parameter space\n",
    "    @param cv: number of cv folds\n",
    "    \"\"\"\n",
    "    kde = KernelDensity()\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=kde,\n",
    "        param_distributions=param_space,\n",
    "        n_iter=100,\n",
    "        cv=cv,\n",
    "        scoring=None # use estimators internal scoring function, i.e. the log-probability of the validation set for KDE\n",
    "    )\n",
    "\n",
    "    search.fit(X_train)\n",
    "    return search.best_params_, search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed34cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Run the code below to perform hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01513b81",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "params, kde = hyperopt_by_score(X_train, param_space)\n",
    "\n",
    "print('Best parameters:')\n",
    "for key in params:\n",
    "    print('{}: {}'.format(key, params[key]))\n",
    "\n",
    "test_scores = -kde.score_samples(X_test)\n",
    "test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "\n",
    "curves = evaluate(y_test, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_kde(kde, params['bandwidth'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d598cda",
   "metadata": {},
   "source": [
    "### Exercise: Isolate anomalies in house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b4783",
   "metadata": {},
   "source": [
    "You are a company resposible to estimate house prices around Ames, Iowa, specifically around college area. But there is a problem: houses from a nearby area, 'Veenker', are often included in your dataset. You want to build an anomaly detection algorithm that filters one by one every point that comes from the wrong neighborhood. You have been able to isolate an X_train dataset which, you are sure, contains only houses from College area. Following the previous example, test your ability to isolate anomalies in new incoming data (X_test) with KDE.\n",
    "\n",
    "Advanced exercise:\n",
    "What happens if the contamination comes from other areas? You can choose among the following names:\n",
    "\n",
    "OldTown, Veenker, Edwards, MeadowV, Somerst, NPkVill, BrDale, Gilbert, NridgHt, Sawyer, Blmngtn, Blueste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_test = get_house_prices_data(neighborhood = 'CollgCr', anomaly_neighborhood='Veenker')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total data\n",
    "train_test_data = X_train.append(X_test, ignore_index=True)\n",
    "y_total = [0] * len(X_train) + y_test\n",
    "\n",
    "fig = px.scatter_3d(train_test_data, x='LotArea', y='OverallCond', z='SalePrice', color=y_total)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc80809",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336ebbd",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# When data are highly in-homogeneous, like in this case, it is often beneficial \n",
    "# to rescale them before applying any anomaly detection or clustering technique.\n",
    "scaler = MinMaxScaler()\n",
    "X_train_rescaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159cf66",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'], # Add available kernels\n",
    "    'bandwidth': np.linspace(0.1, 10, 100), # Define Search space for bandwidth parameter\n",
    "}\n",
    "params, kde = hyperopt_by_score(X_train_rescaled, param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6217804",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "print('Best parameters:')\n",
    "for key in params:\n",
    "    print('{}: {}'.format(key, params[key]))\n",
    "\n",
    "X_test_rescaled = scaler.transform(X_test)\n",
    "test_scores = -kde.score_samples(X_test_rescaled)\n",
    "test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "curves = evaluate(y_test, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2238bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Curse of Dimensionality\n",
    "The flexibility of KDE comes at a price. The dependency on the dimensionality of the data is quite unfavorable.\n",
    "\n",
    "\n",
    "*Theorem* [Stone, 1982]\n",
    "Any estimator that is consistent$^*$ with the class of all $k$-fold differentiable pdfs over $\\mathbb{R}^d$ has a\n",
    "convergence rate of at most\n",
    "\n",
    "$$\n",
    "\\frac{1}{n^{\\frac{k}{2k+d}}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$^*$Consistency = for all pdfs $p$ in the class: $\\lim_{n\\to\\infty}|KDE_h(x, D) - p(x)|_\\infty = 0$ with probability $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65268b84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "- The very slow convergence in high dimensions does not necessary mean that we will see bad results in high dimensional anomaly detection with KDE.\n",
    "- Especially if the anomalies are very outlying.\n",
    "- However, in cases where contours of the nominal distribution are non-convex we can run into problems.\n",
    "\n",
    "We take a look at a higher dimensional version of out previous data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8b1a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dists = create_distributions(dim=3)\n",
    "\n",
    "distribution_with_anomalies = contamination(\n",
    "    nominal=dists['Sinusoidal'],\n",
    "    anomaly=dists['Blob'],\n",
    "    p=0.01\n",
    ")\n",
    "\n",
    "sample = distribution_with_anomalies.sample(500)\n",
    "\n",
    "y = sample[0]\n",
    "X = sample[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7d48b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Fit KDE on high dimensional examples \n",
    "rocs = []\n",
    "auprs = []\n",
    "bandwidths = []\n",
    "\n",
    "param_space = {\n",
    "        'kernel': ['gaussian'],\n",
    "        'bandwidth': np.linspace(0.1, 100, 1000), # Define Search space for bandwidth parameter\n",
    "    }\n",
    "\n",
    "kdes = {}\n",
    "dims = np.arange(2,16)\n",
    "for d in tqdm(dims):\n",
    "    # Generate d dimensional distributions\n",
    "    dists = create_distributions(dim=d)\n",
    "\n",
    "    distribution_with_anomalies = contamination(\n",
    "        nominal=dists['Sinusoidal'],\n",
    "        anomaly=dists['Blob'],\n",
    "        p=0\n",
    "    )\n",
    "\n",
    "    # Train on clean data\n",
    "    sample_train = dists['Sinusoidal'].sample(500)\n",
    "    X_train = sample_train[-1].numpy()\n",
    "    # Test data\n",
    "    sample_test = distribution_with_anomalies.sample(500)\n",
    "    X_test = sample_test[-1].numpy()\n",
    "    y_test = sample_test[0].numpy()\n",
    "\n",
    "    # Optimize bandwidth\n",
    "    params, kde = hyperopt_by_score(X_train, param_space)\n",
    "    kdes[d] = (params, kde)\n",
    "    \n",
    "    bandwidths.append(params['bandwidth'])\n",
    "\n",
    "    test_scores = -kde.score_samples(X_test)\n",
    "    test_scores = np.where(test_scores == np.inf, np.max(test_scores[np.isfinite(test_scores)])+1, test_scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc679493",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot cross section of pdf \n",
    "fig, axes = plt.subplots(nrows=2, ncols=7, figsize=(15, 5))\n",
    "for d, axis in tqdm(list(zip(kdes, axes.flatten()))):\n",
    "    \n",
    "    params, kde = kdes[d]\n",
    "\n",
    "    lin = np.linspace(-10, 10, 50)\n",
    "    grid_points = list(it.product(*([[0]]*(d-2)), lin, lin))\n",
    "    ys, xs = np.meshgrid(lin, lin)\n",
    "    # The score function of sklearn returns log-densities\n",
    "    scores = np.exp(kde.score_samples(grid_points)).reshape(50, 50)\n",
    "    colormesh = axis.contourf(xs, ys, scores)\n",
    "    axis.set_title(\"Dim = {}\".format(d))\n",
    "    axis.set_aspect('equal')\n",
    "    \n",
    "\n",
    "# Plot evaluation\n",
    "print('Crossection of the KDE at (0,...,0, x, y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020500ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robustness\n",
    "Another drawback of KDE in the context of anomaly detection is that it is not robust against contamination of the data\n",
    "\n",
    "\n",
    "**Definition**\n",
    "The *breakdown point* of an estimator is the smallest fraction of observations that need to be changed so that we can\n",
    "move the estimate arbitrarily far away from the true value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf4c13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**: The sample mean has a breakdown point of $0$. Indeed, for a sample of $x_1,\\ldots, x_n$ we only need to\n",
    "change a single value in order to move the sample mean in any way we want. That means that the breakdown point is\n",
    "smaller than $\\frac{1}{n}$ for every $n\\in\\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efce06e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robust Statistics\n",
    "There are robust replacements for the sample mean:\n",
    "- Median of means: Split the dataset into $S$ equally sized subsets $X_1,\\ldots, X_S$ and compute\n",
    "$\\mathrm{median}(\\overline{X_1},\\ldots, \\overline{X_S})$\n",
    "- M-estimation: The mean in a normed vector space is the value that minimizes the squared distances\n",
    "<center>\n",
    "$\\overline{X} = \\min_{y}\\sum_{x\\in X}|x-y|^2$\n",
    "</center>\n",
    "M-estimation replaces the quadratic loss with a more robust loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5655c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Huber loss\n",
    "Switch from quadratic to linear loss at prescribed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6861d26",
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def huber(error: float, threshold: float):\n",
    "    \"\"\"Huber loss\n",
    "    \n",
    "    @param error: base error\n",
    "    @param threshold: threshold for linear transition\n",
    "    \"\"\"\n",
    "    test = (np.abs(error) <= threshold)\n",
    "    return (test * (error**2)/2) + ((1-test)*threshold*(np.abs(error) - threshold/2))\n",
    "\n",
    "x = np.linspace(-5, 5)\n",
    "y = huber(x, 1)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set_title(\"Huber Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017109f",
   "metadata": {},
   "source": [
    "### Hampel loss\n",
    "More complex loss function. Depends on 3 parameters 0 < a < b< r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f95fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def single_point_hampel(error: float, a: float, b: float, r: float):\n",
    "    \"\"\"Hampel loss\n",
    "    \n",
    "    @param error: base error\n",
    "    @param a: 1st threshold parameter\n",
    "    @param b: 2nd threshold parameter\n",
    "    @param r: 3rd threshold parameter\n",
    "    \"\"\"\n",
    "    if abs(error) <= a:\n",
    "        return error**2/2\n",
    "    elif a < abs(error) <= b:\n",
    "        return (1/2 *a**2 + a* (abs(error)-a))\n",
    "    elif  b < abs(error) <= r:\n",
    "        return a * (2*b-a+(abs(error)-b) * (1+ (r-abs(error))/(r-b)))/2\n",
    "    else:\n",
    "        return a*(b-a+r)/2\n",
    "\n",
    "hampel = np.vectorize(single_point_hampel)\n",
    "\n",
    "x = np.linspace(-10.1, 10.1)\n",
    "y = hampel(x, a=1.5, b=3.5, r=8)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set_title(\"Hampel Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed645c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KDE is a Mean\n",
    "\n",
    "<img src=\"_static/images/kernels.png\" align=\"right\" width=\"400\" style=\"margin: 150px 75px\">\n",
    "\n",
    "\n",
    "\n",
    "**Kernel as scalar product:**\n",
    "\n",
    "- Let $K$ be a radial monotonic$^\\ast$ kernel over $\\mathbb{R}^n$.\n",
    "- For $x\\in\\mathbb{R}^n$ let $\\phi_x = K(\\cdot, x)$.\n",
    "- Vector space over the linear span of $\\{\\phi_x \\mid x\\in\\mathbb{R}^n\\}$:\n",
    "    - Pointwise addition and scalar multiplication.\n",
    "- Define the scalar product $\\langle \\phi_x, \\phi_y\\rangle = K(x,y)$.\n",
    "- Advantage: Scalar product is computable\n",
    "- Call this the reproducing kernel Hilbert space (RKHS) of $K$.\n",
    "- $\\mathrm{KDE}_h(\\cdot, D) = \\frac{1}{N}\\sum_{i=1}^N K_h(\\cdot, x_i) = \\frac{1}{N}\\sum_{i=1}^N\\phi_{x_i}$\n",
    "    - where $K_h(x,y) = \\frac{1}{h}K\\left(\\frac{|x-y|}{h}\\right)$\n",
    "\n",
    "\n",
    "\n",
    "$^*$All kernels that we have seen are radial and monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e6487",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "We compare the performance of different approaches to recover the nominal distribution under contamination.\n",
    "Here, we use code by [Humbert et al.](https://github.com/lminvielle/mom-kde) to replicate\n",
    "the results in the referenced paper on median-of-mean KDE. More details on rKDE can instead be found in this paper by [Kim and Scott.](https://arxiv.org/abs/1107.3133#:~:text=We%20propose%20a%20method%20for,ideas%20from%20classical%20M%2Destimation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58243128",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "#   Parameters\n",
    "# =======================================================\n",
    "algos = [\n",
    "    'kde',\n",
    "    'mom-kde', # Median-of-Means\n",
    "    'rkde-huber', # robust KDE with huber loss\n",
    "    'rkde-hampel', # robust KDE with hampel loss\n",
    "]\n",
    "\n",
    "dataset = 'house-prices'\n",
    "dataset_options = {'neighborhood': 'CollgCr', 'anomaly_neighborhood': 'Edwards'}\n",
    "\n",
    "outlierprop_range = [0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "kernel = 'gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da628e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "auc_scores = perform_rkde_experiment(\n",
    "    algos,\n",
    "    dataset,\n",
    "    dataset_options,\n",
    "    outlierprop_range,\n",
    "    kernel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ed959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "for algo, algo_data in auc_scores.groupby('algo'):\n",
    "    x = algo_data.groupby('outlier_prop').mean().index\n",
    "    y = algo_data.groupby('outlier_prop').mean()['auc_anomaly']\n",
    "    ax.plot(x, y, 'o-', label=algo)\n",
    "plt.legend()\n",
    "plt.xlabel('outlier_prop')\n",
    "plt.ylabel('auc_score')\n",
    "plt.title('Comparison of rKDE against contamination')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd659a73",
   "metadata": {},
   "source": [
    "Try using different neighborhoods for contamination. Which robust KDE algorithm performs better overall? Choose among the following options:\n",
    "\n",
    "OldTown, Veenker, Edwards, MeadowV, Somerst, NPkVill, BrDale, Gilbert, NridgHt, Sawyer, Blmngtn, Blueste\n",
    "\n",
    "You can also change the kernel type: gaussian, tophat, epechenikov, exponential, linear or cosine, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76312d0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary\n",
    "- Kernel density estimation is a non-parametric method to estimate a pdf from a sample.\n",
    "- Bandwidth is the most important parameter.\n",
    "- Converges to the true pdf if $n\\to\\infty$.\n",
    "    - Convergence exponentially depends on the dimension.\n",
    "- KDE is sensitive to contamination:\n",
    "    - In a contaminated setting one can employ methods from robust statistics to obtain robust estimates.\n",
    "    \n",
    "## Implementations\n",
    "- Sklearn: [sklearn.neighbors.KernelDensity](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)\n",
    "- Statsmodels: [statsmodels.nonparametric.kernel_density.KDEMultivariate](https://www.statsmodels.org/dev/generated/statsmodels.nonparametric.kernel_density.KDEMultivariate.html)\n",
    "- FastKDE: [link](https://pypi.org/project/fastkde/), offers automatic bandwidth and kernel selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd207a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dab0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
