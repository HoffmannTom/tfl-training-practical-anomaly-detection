{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The output of this cell should be committed to the repo such that the styles are applied after opening the notebook \n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "with open(\"rise.css\", \"r\") as f:\n",
    "    styles = f.read()\n",
    "HTML(f'<style>{styles}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aai_presentation_first_slide.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Taxonomy of Anomaly Detection Approaches</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of approaches to AD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Anomaly detection as a whole does not really have a common foundation.\n",
    "\n",
    "Rather, there are loosely related approaches to it and methods from those approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For actual applications of anomaly detection the structure of the data and the problem\n",
    "at hand are of extreme importance for choosing the right method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes, hand designed rules might work better than fancy algorithms! Generally, a thorough\n",
    "statistical analysis of the data should be performed before using \"heavier machinery\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below we present a small of taxonomy of different approaches to AD with some representative methods. However, the lines between these approaches can become quite blurry at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distance based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Philosophy__: a point is an outlier if it has few neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Distance to $k$-th nearest neighbor ($k$ needs to be determined), clustering (together Mahalanobis distance), local outlier factor (LOF), the matrix-profile for time series etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Philosophy__: since most data points are normal, we can fit a probabilistic model to \"normality\". A point is an outlier if it has low probability under the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kernel density estimation, gaussian mixtures, extreme value theory, GAN-based anomaly detection, time series forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We already see blurry lines - gaussian mixtures could be considered a distance based method for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Subspace based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Philosophy__: The space where data lives can be partitioned into a normal region and an abnormal one. This partitioning might happen in a lower dimensional subspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Isolation trees/forests, one class SVM, genetic algorithms, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reconstruction based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Philosophy__: We learn an encoding (or projection) to a low dimensional space and a corresponding decoding of our data. Since we mostly have normal data points, it will be more efficient for them than for the anomalies. Thus, anomalies will have higher reconstruction error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Autoencoder, variational autoencoder, assotiative memory models (Hopfield networks) etc, subspace methods, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again blurry lines: is PCA reconstruction based or probabilistic? The variational autoencoder also gives a probabilistic model for data as a byproduct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Philosophy__: We can just train a classifier to predict a binary (or more complex) label. The downside here is that this requires labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The typical classifiers with tricks for dealing with unbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other approaches and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Information based (removing anomalies should drastically decrease information content of the data set), domain specific, combination of different methods etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We cannot possibly try to cover all the approaches above. Instead, we will demonstrate a few techniques that often work well in practice and then dive deeper into two general topics: _time series_ and _extreme value theory_. \n",
    "\n",
    "The topics covered there will be useful in a variety of situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
