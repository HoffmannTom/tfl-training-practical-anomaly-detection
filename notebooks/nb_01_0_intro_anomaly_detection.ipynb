{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_anomaly_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Anomaly Detection\n",
    "<img src=\"_static/images/aai_presentation_first_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from tfl_training_anomaly_detection.exercise_tools import evaluate, visualize_mahalanobis\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What is an Anomaly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/fish.jpg\" align=\"center\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grubbs, 1969:\n",
    "> An outlying observation, or \"outlier,\" is one that appears to deviate markedly from other members of\n",
    "> the sample in which it occurs.\n",
    "\n",
    "Hawkins, 1980:\n",
    "> An outlier is an observation that deviates so much from other observations as to arouse suspicion that it was\n",
    "> generated by a different mechanism.\n",
    "\n",
    "Chandola et al., 2009:\n",
    ">Anomalies are patterns in data that do not conform to a well defined notion of normal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Anomalies Can be Hard to Detect\n",
    "<center>\n",
    "<img src=\"_static/images/waldo.jpg\" align=\"center\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Practical Relevance of Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predictive Maintenance\n",
    "- Determine condition of in-service equipment\n",
    "- Optimize maintenance cycle\n",
    "- Too frequent inspections cause unnecessary costs and downtime\n",
    "- Too infrequent inspections can lead to failures or even breaking of the equipment\n",
    "<center>\n",
    "<img src=\"_static/images/failure.jpg\" align=\"center\" width=\"200\">\n",
    "</center>\n",
    "\n",
    "**Anomaly Detection:** Sensory data can provide valuable information about the condition of the component. Increasingly\n",
    "abnormal readings may indicate a wear of the equipment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fraud Detection\n",
    "- Identify fraudulent transactions, e.g. credit card\n",
    "- Prevent criminal activities\n",
    "- Avoid financial or other  damages for the involved parties\n",
    "\n",
    "<center>\n",
    "<img src=\"_static/images/fraud.jpg\" align=\"center\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "**Anomaly Detection:** Fraudulent transactions can often be identified through unusual destinations, amounts,\n",
    "or network topology (over several transactions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intrusion Detection\n",
    "- Detect attacks against a network\n",
    "- Protect nodes against unauthorized access\n",
    "\n",
    "<center>\n",
    "<img src=\"_static/images/cyber.jpg\" align=\"center\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "**Anomaly Detection**\n",
    "Malicious connections can leaf unusual footprints, e.g., used protocol, ports, number of packages, IP, duration, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Relevance of Unsupervised Machine Learning in AD\n",
    "- Due to the difficulty of identifying anomalies one often has no labeled data available \n",
    "- Even if labels are available, anomalies are rare and the data sets are heavily imbalanced  \n",
    "- Often, we don't want to restrict the system to anomalies that we have encountered in the past\n",
    "- The information that is available heavily influences the applicable techniques:\n",
    "    - Is the distribution of nominal data known?\n",
    "    - Is there clean data (without anomalies) for training?\n",
    "    - Do we have labeled anomalies for evaluation?\n",
    "    - How large is the proportion of anomalies?\n",
    "    - How much noise is in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Question\n",
    "Where do you think you can benefit from anomaly detection?\n",
    "- Which problem do you want to solve?\n",
    "- How does it translate into an anomaly detection problem?\n",
    "- What data is available (dimensionality, time dependence, $\\ldots$)?\n",
    "    - Clean data (without anomalies) available?\n",
    "    - Labeled anomalies available?\n",
    "    - Proportion of outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contamination Framework\n",
    "<img align=\"right\"  src=\"_static/images/contamination.png\" style=\"margin: 0px 100px\" width=\"500\">\n",
    "\n",
    "- Unsupervised Scenario\n",
    "- Two distributions:\n",
    "    - $F_0$ generates normal points\n",
    "    - $F_1$ generates anomalies\n",
    "    - $p$ relative frequency of $F_1$\n",
    "- Data set $D \\stackrel{\\text{IID}}{\\sim} F=(1-p)F_0 + pF_1$\n",
    "\n",
    "**Task:** Estimate if given $x$ is anomalous\n",
    "\n",
    "**Assumptions:**\n",
    "- *Few*: $p \\ll 1/2$\n",
    "- *Outlying*: $F_0$ and $F_1$ do not overlap too much\n",
    "- *Sparse*: $F_1$ is less clustered than $F_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Does the Contamination Framework Always Apply?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No!\n",
    "- We might have clean data without anomalies available for training\n",
    "- In an adversarial scenario, like fraud detection, the opponent might change her behavior over time to evade detection\n",
    "  $\\Rightarrow$ $F_1$ might not be well-defined\n",
    "- The degree to which the three assumptions are true can vary for each specific problem\n",
    "- Some assumptions might even be false in some scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation Metrics\n",
    "- Accuracy is not a good measure in anomaly detection:\n",
    "    - $1\\%$ anomalies $\\Rightarrow$ always predicting nominal gives $99\\%$ accuracy!\n",
    "- Better measures are precision, recall and $F_1$\n",
    "- The confusion matrix divides a test set according to the predictions and ground truth \n",
    "\n",
    "|Confusion Matrix |Actual Nominal| Actual Anomaly|\n",
    "|---|-----------|-------|\n",
    "|__Predicted Nominal__ |True Negative (TN)  | False Negative (FN)  |\n",
    "|__Predicted Anomaly__     |False Positive (FP)| True Positive (TP)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision, Recall \n",
    "- Precision is defined as \n",
    "\n",
    "$$\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$$\n",
    "\n",
    "It estimates the probability that an observation really is anomalous given that the detection system predicted it to be. \n",
    "- Recall is defined as \n",
    "\n",
    "$$\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$$\n",
    "\n",
    "It estimates the probability that an observation will be predicted to be anomalous given that it really is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $F_1$ Score\n",
    "- $F_1$ is defined as the harmonic mean of precision and recall \n",
    "\n",
    "$$2\\cdot \\frac{\\text{Precision}\\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$ \n",
    "\n",
    "It balances between precision and recall.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluating Thresholds\n",
    "- Most anomaly detection algorithms output an anomaly score where higher values mean more anomalous. \n",
    "- We need to set a decision threshold $\\tau$ in order to compute precision, recall and $F_1$. \n",
    "<img src=\"_static/images/example_roc_pr.png\" width=\"400\" align=\"right\">\n",
    "- The precision-recall (PR) curve plots the pairs \n",
    "\n",
    "$$\\{(\\text{Recall}(\\tau), \\text{Precision}(\\tau)) \\mid \\tau_{\\text{min}} \\leq \\tau \\leq \\tau_{\\text{max}}\\}$$\n",
    "\n",
    "\n",
    "- The receiver-operator-characteristics curve (ROC) plots the true positive rate (TPR) against the false positive rate (FPR) for the possible thresholds\n",
    "    - $\\mathrm{TPR(\\tau)} = \\frac{\\mathrm{TP(\\tau)}}{\\mathrm{TP(\\tau)}+\\mathrm{FN(\\tau)}}$\n",
    "    - $\\mathrm{FPR(\\tau)} = \\frac{\\mathrm{FP(\\tau)}}{\\mathrm{FP}(\\tau)+\\mathrm{TN}(\\tau)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Matrix\n",
    "Choosing the optimal threshold does not only depend on the values of our metrics but also on the cost associated to the confusion matrix. Similarly to precision, recall, etc. the confusion matrix has to be understood as a function of the threshold. For each threshold we obtain different numbers of true positives, false positives... . The associated costs e.g. for the false positives are the expected costs that a falsely positive prediction generates (profits are represented as negative costs). Other than the confusion matrix, the cost matrix does not depend on $\\tau$. \n",
    "\n",
    "|Cost Matrix |Actual Nominal| Actual Anomaly|\n",
    "|---|---|---|\n",
    "|__Predicted Nominal__ |Cost of  TN (CTN)| Cost of FN(CFN)|\n",
    "|__Predicted Anomaly__     |Cost of FP (CFP)|Cost of TP (CTP)|\n",
    "\n",
    "Our goal is to set the threshold such that the expected empirical costs will be minimized\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tau_{\\text{opt}}=\\arg\\min_\\tau \\frac{\\mathrm{TP}(\\tau)\\cdot \\mathrm{CTP} + \\mathrm{FP}(\\tau)\\cdot \\mathrm{CFP} + \\mathrm{TN}(\\tau)\\cdot \\mathrm{CTN} + \\mathrm{FN}(\\tau)\\cdot \\mathrm{CFN}}{N}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Our First Anomaly Detection Approach\n",
    "Let's have a look at a simple probabilistic anomaly score.\n",
    "- If the distribution of nominal data is known then we can use $-\\log p(x)$, also known as the surprise. \n",
    "- If only the covariance $\\Sigma$ and the mean $\\mu$ is known, we can compute the Mahalanobis distance to the mean $\\sqrt{(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$ can be used.\n",
    "    - Only applicable if nominal distribution is unimodally centered around the mean.\n",
    "    - Extension for mixture models with means $\\mu_1,\\ldots,\\mu_k$ and covariance matrices  $\\Sigma_1,\\ldots,\\Sigma_k$: $\\min_{1\\leq i \\leq k}\\sqrt{(x-\\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation of Mahalanobis Distance\n",
    "\n",
    "- The Mahalanobis distance is motivated by the surprise of a Gaussian:\n",
    "\\begin{align*}\n",
    "-\\log p(x) &= -\\log \\frac{1}{(2\\pi)^{\\frac{m}{2}}\\sqrt{|\\det(\\Sigma)|}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\\\\ \n",
    "&= \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu) + c\n",
    "\\end{align*}\n",
    "- Since monotonous transformations (such as $\\sqrt{\\cdot}$ or adding a constant) don't change the outlier ranking this is equivalent to the Mahalanobis distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "Try the outlier scores for yourself in a simple synthetic scenario. We have prepared the function evaluate for you. Try to find the optimal threshold for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nominal = np.random.normal(0, [1, 1.5], size=(300, 2))\n",
    "anomaly = np.random.normal(5, 2, size=(10, 2))\n",
    "\n",
    "data = np.concatenate([nominal, anomaly], axis=0)\n",
    "y = np.zeros(310)\n",
    "y[-10:] = 1\n",
    "\n",
    "plt.scatter(data[:, 0], data[:,1], c=y)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Fit a Gaussian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mu = data.mean(axis=0)\n",
    "Sigma_diag = data.std(axis=0) # assumes independant components\n",
    "print('Mean: {}\\nStd: {}'.format(mu, Sigma_diag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Question\n",
    "How did the contamination influence the parameter estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Compute scores and evaluate**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Mahalanobis distance from the mean of N(mu, Sigma)\n",
    "scores = np.sqrt(((data - mu) * (1/Sigma_diag) * (data - mu)).sum(axis=1)) \n",
    "curves = evaluate(y, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Choose a threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_mahalanotis(data, y, scores, mu, sigma_diag, thr):\n",
    "    _, axes = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Visualize Data\n",
    "    scatter_gt = axes.scatter(data[:, 0], data[:,1], c=y)\n",
    "    plt.scatter(mu[0], mu[1], color='red')\n",
    "    axes.set_title('Ground Truth')\n",
    "    handles, _ = scatter_gt.legend_elements()\n",
    "    axes.legend(handles, ['Nominal', 'Anomaly'])\n",
    "    axes.set_aspect('equal')\n",
    "    # Draw descicion contour\n",
    "    descion_border = Ellipse(\n",
    "        mu,\n",
    "        width=2*np.sqrt(sigma_diag[0])*thr,\n",
    "        height=2*np.sqrt(sigma_diag[1])*thr,\n",
    "        color='red',\n",
    "        fill=False\n",
    "    )\n",
    "    axes.add_patch(descion_border)\n",
    "    \n",
    "    # Evaluate threshold\n",
    "    y_pred = scores >  thr\n",
    "\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    \n",
    "    axes.set_title(\"Precision: {}\\nRecall: {}\\nF1: {}\".format(precision, recall, f1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "thr = None\n",
    "\n",
    "@interact(threshold=(0., 6.))\n",
    "def set_threshold(threshold):\n",
    "    global thr\n",
    "    thr = threshold\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "visualize_mahalanobis(data, y, scores, mu, Sigma_diag, thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Task: Find optimal threshold and evaluate on test set.\n",
    "Choose good threshold. You may write additional code to determine the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "thr_opt = 3.2 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_test = np.concatenate([np.random.normal(0, [1, 1.5], size=(300, 2)), np.random.normal(3, 1.5, size=(10, 2))])\n",
    "\n",
    "y_test = np.zeros(data_test.shape[0])\n",
    "y_test[-10:] = 1\n",
    "\n",
    "scores_test = np.sqrt(((data_test - mu) * (1/Sigma_diag) * (data_test - mu)).sum(axis=1)) \n",
    "\n",
    "visualize_mahalanotis(data_test, y_test, scores_test, mu, Sigma_diag, thr_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "- Anomalies are patterns in data that do not conform to a well defined notion of normal behavior.\n",
    "- Detecting anomalies can be very valuable in a broad spectrum of industry sectors and company divisions.\n",
    "- Anomaly detection uses mostly unsupervised techniques.\n",
    "- Outlier scores measure the degree of outlyingness. \n",
    "- If some statistical properties of the nominal distribution are known then the surprise or the Mahalanobis distance can be used as an outlier score.\n",
    "- Evaluation metrics: precision, recall, $F_1$, ROC (AUC), PR (AUC). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
