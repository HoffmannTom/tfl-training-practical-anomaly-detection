{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c634d79e",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tfl_training_anomaly_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596df825",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "  position: relative;\n",
       "  top: -50%;\n",
       "  margin-left: 5%;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158112af",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c8e783",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2f648",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Reconstruction Error\n",
    "<img src=\"_static/images/aai_presentation_first_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d462cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tfl_training_anomaly_detection.exercise_tools import (\n",
    "    evaluate, \n",
    "    get_kdd_data, \n",
    "    get_house_prices_data, \n",
    "    create_distributions, \n",
    "    contamination, \n",
    "    perform_rkde_experiment, \n",
    "    get_mnist_data\n",
    ")\n",
    "from tfl_training_anomaly_detection.vae import VAE, build_decoder_mnist, build_encoder_minst, build_contaminated_minst\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287222a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Reconstruction Error\n",
    "**Idea:** Embed the data into low dimensional space and reconstruct it again.\n",
    "\t\t\tGood embedding of nominal data $\\Rightarrow$ high reconstruction error indicates anomaly.\n",
    "\n",
    "**Autoencoder:**\n",
    "- Parametric family of encoders: $f_\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{\\text{low}}$\n",
    "- Parametric family of decoders: $g_\\theta: \\mathbb{R}^{\\text{low}} \\to \\mathbb{R}^{d}$\n",
    "- Reconstruction error of $(f_\\phi, g_\\theta)$ on $x$: $|x - g_\\theta(f_\\phi(x))|$\n",
    "- Given data set $D$, find $\\phi,\\theta$ that minimize $\\sum_{x\\in D} L(|x- g_\\theta(f_\\phi(x))|) $\n",
    "  for some loss function $L$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5094e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualization\n",
    "<center>\n",
    "<img src=\"_static/images/autoencoder.png\" align=\"center\" width=\"400\">\n",
    "    Autoencoder Schema\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a43e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "Neural networks are very well suited for finding low dimensional representations of data. Hence they are a popular choice for the encoder and the decoder.\n",
    "\n",
    "\n",
    "**Artificial Neuron with $N$ inputs:** $y = \\sigma\\left(\\sum_i^N w_i X_i + b\\right)$\n",
    "\n",
    "- $\\sigma$: nonlinear activation-function (applied component wise).\n",
    "- $b$ bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658847c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table style=\"background-color:#FFFFFF;\">\n",
    "    <tr>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/neuron.png\" width=\"200\"></td>\n",
    "        <td style=\"background-color:#FFFFFF;\"><img src=\"_static/images/activations.png\"  width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" style=\"background-color:#FFFFFF;\"><center>Isolation depth of nominal point and anomaly</center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09243719",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks combine many artificial neurons into a complex network. These networks are usually organized in layers\n",
    "where the result of each layer is the input for the next layer. Some commonly used layers are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376f048",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/nn_layers.png\" align=\"center\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8794297",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variational Autoencoders\n",
    "An important extension of autoencoders that relates the idea to density estimation.\n",
    "More precisely, we define a generative model for our data using latent variables and combine the maximum likelihood\n",
    "estimation of the parameters with a simultaneous posterior estimation of the latents through amortized stochastic\n",
    "variational inference. We use a decoder network to transform the latent variables into the data distribution, and an\n",
    "encoder network to compute the posterior distribution of the latents given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5efeac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:**\n",
    "\n",
    "\n",
    "The model uses an observed variable $X$ (the data) and a latent variable $Z$ (the defining features of $X$). We assume\n",
    "both $P(Z)$ and $P(X\\mid Z)$ to be normally distributed. More precisely\n",
    "\n",
    "- $P(Z) = \\mathcal{N}(0, I)$\n",
    "- $P(X\\mid Z) = \\mathcal{N}(\\mu_\\phi(Z), I)$\n",
    "\n",
    "where $\\mu_\\phi$ is a neural network parametrized with $\\phi$.\n",
    "We use variational inference to perform posterior inference on $Z$ given $X$. We assume that the distribution $P(Z\\mid X)$\n",
    "to be relatively well approximated by a Gaussian and use the posterior approximation:\n",
    "- $q(X\\mid Z) = \\mathcal{N}(\\mu_\\psi(X), \\sigma_\\psi(X))$\n",
    "\n",
    "$\\mu_\\psi$ and $\\sigma_\\psi$ are neural networks parameterized with $\\psi$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d013d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"_static/images/vae_schema.png\" align=\"center\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81661bf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a data set $D$ we minimize the (amortized) Kullback-Leibler divergence between our posterior approximation and the\n",
    "true posterior:\n",
    "\\begin{align*}\n",
    "  D_{KL}(q(z\\mid x),p(z\\mid x)) &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(z \\mid X)}\\right)\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid X)}\\left[\\log\\left(\\frac{q(z \\mid x)}{\\frac{p(x \\mid z)p(z)}{p(x)}}\\right)\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right) + \\log(p(x))\\right] \\\\\n",
    "    &= E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right)\\right]  + E_{x\\sim X}[\\log(p(x))]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now we can define\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{ELBO}(q(z\\mid x),p(z\\mid x)) &:= E_{x\\sim X}[\\log(p(x))] - D_{KL}(q(z\\mid x),p(z\\mid x)) \\\\\n",
    "        &= -E_{x\\sim X, z\\sim q(Z\\mid x)}\\left[\\log\\left(\\frac{q(z \\mid x)}{p(x \\mid z)p(z)}\\right)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Note that we can evaluate the expression inside the expectation of the final RHS of the\n",
    "equation and we can obtain unbiased estmates of the expectation via sampling.\n",
    "Let us further try to understand the ELBO as an optimization objective. On one hand, maximizing the ELBO with respect to the parameters in $q$ is equivalent to\n",
    "minimizing the KL divergence between $p$ and $q$. On the other hand, maximizing the ELBO with\n",
    "respect to the parameters in $p$ can be understood as raising a lower bound for the likelihood of the\n",
    "generative model $p(x)$. Hence, the optimization tries to find an encoder and a decoder pair such that\n",
    "it simultaneously provides a good generative explanation of the data and a good approximation of the posterior\n",
    "distribution of the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131628c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8c5a6",
   "metadata": {},
   "source": [
    "# The MNIST Data Set\n",
    "MNIST is one of the most iconic data sets in the history of machine learning.\n",
    "It contains 70000 samples of $28\\times 28$ grayscale images of handwritten digits.\n",
    "Because of its moderate complexity and good visualizability it is well suited to study the behavior of machine learning\n",
    "algorithms in higher dimensional spaces.\n",
    "\n",
    "While originally created for classification (optical character recognition), we can build an anomaly detection data set\n",
    "by corrupting some of the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28979a57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Pre-processing\n",
    "We first need to obtain the MNIST data set and prepare an anomaly detection set from it.\n",
    "Note that the data set is n row vector format.\n",
    "Therefore, we work with $28\\times 28 = 784$ dimensional data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2508225",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load MNIST Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d8450",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mnist = get_mnist_data()\n",
    "\n",
    "data = mnist['data']\n",
    "print('data.shape: {}'.format(data.shape))\n",
    "target = mnist['target'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6bde9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Build contaminated Data Sets\n",
    "We prepared a function that does the job for us.\n",
    "It corrupts a prescribed portion of the data by introducing a rotation, noise or a blackout of some part of the image.\n",
    "\n",
    "First, we need to transform the data into image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3762",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X = data.reshape(-1, 28, 28, 1)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6c089",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Train/Test-Split\n",
    "We will only corrupt the test set, hence we will perform the train-test split beforehand.\n",
    "We separate a relatively small test set so that we can use as much as possible from the data to obtain high quality\n",
    "representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb9c57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "X_train, X_test, target_train, target_test = train_test_split(X, target, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad3d43",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = build_contaminated_minst(X_test)\n",
    "\n",
    "# Visualize contamination\n",
    "anomalies = X_test[y_test != 0]\n",
    "selection = np.random.choice(len(anomalies), 25)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(5, 5))\n",
    "for img, ax in zip(anomalies[selection], axes.flatten()):\n",
    "    ax.imshow(img, 'gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c459a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Autoencoder\n",
    "Let us finally train an autoencoder model. We replicate the model given in the\n",
    "[Keras documentation](https://keras.io/examples/generative/vae/) and apply it in a synthetic outlier detection scenario\n",
    "based on MNIST.\n",
    "\n",
    "in the vae package we provide the implementation of the VAE. Please take a look into the source code to see how\n",
    "the minimization of the KL divergence is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af1c41",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90996a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "vae = VAE(decoder=build_decoder_mnist(latent_dim=latent_dim), encoder=build_encoder_minst(latent_dim=latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb89bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect model architecture\n",
    "vae.encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b219e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect model architecture\n",
    "vae.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b43aff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "n_epochs = 30\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=.001))\n",
    "history = vae.fit(X_train, epochs=n_epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1519875",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Inspect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab41fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(vae: VAE, n: int=10, figsize: float=10):\n",
    "    \"\"\"Plot sample images from 2D slices of latent space\n",
    "    \n",
    "    @param vae: vae model\n",
    "    @param n: sample nXn images per slice\n",
    "    @param figsize: figure size\n",
    "    \n",
    "    \"\"\"\n",
    "    for perm in [[0, 1, 2], [1, 2, 0], [2, 1, 0]]:\n",
    "        # display a n*n 2D manifold of digits\n",
    "        digit_size = 28\n",
    "        scale = 1.0\n",
    "        figure = np.zeros((digit_size * n, digit_size * n))\n",
    "        # linearly spaced coordinates corresponding to the 2D plot\n",
    "        # of digit classes in the latent space\n",
    "        grid_x = np.linspace(-scale, scale, n)\n",
    "        grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z_sample = np.array([[xi, yi, 0]])\n",
    "                z_sample[0] = z_sample[0][perm]\n",
    "                x_decoded = vae.decoder.predict(z_sample)\n",
    "                digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "                figure[\n",
    "                    i * digit_size : (i + 1) * digit_size,\n",
    "                    j * digit_size : (j + 1) * digit_size,\n",
    "                ] = digit\n",
    "\n",
    "        plt.figure(figsize=(figsize, figsize))\n",
    "        start_range = digit_size // 2\n",
    "        end_range = n * digit_size + start_range\n",
    "        pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "        sample_range_x = np.round(grid_x, 1)\n",
    "        sample_range_y = np.round(grid_y, 1)\n",
    "        plt.xticks(pixel_range, sample_range_x)\n",
    "        plt.yticks(pixel_range, sample_range_y)\n",
    "        plt.xlabel(\"z[{}]\".format(perm[0]))\n",
    "        plt.ylabel(\"z[{}]\".format(perm[1]))\n",
    "        plt.gca().set_title('z[{}] = 0'.format(perm[2]))\n",
    "        plt.imshow(figure, cmap=\"Greys_r\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0f67d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot_latent_space(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a5b6f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Principal components\n",
    "pca = PCA()\n",
    "latents = vae.encoder.predict(X_train)[2]\n",
    "pca.fit(latents)\n",
    "\n",
    "kwargs = {'x_{}'.format(i): (-1., 1.) for i in range(latent_dim)}\n",
    "\n",
    "\n",
    "@widgets.interact(**kwargs)\n",
    "def explore_latent_space(**kwargs):\n",
    "    \"\"\"Widget to explore latent space from given start position\n",
    "    \"\"\"\n",
    "    center_img = pca.transform(np.zeros([1,latent_dim]))\n",
    "\n",
    "    latent_rep_pca =  center_img + np.array([[kwargs[key] for key in kwargs]])\n",
    "    latent_rep = pca.inverse_transform(latent_rep_pca)\n",
    "    img = vae.decoder(latent_rep).numpy().reshape(28, 28)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.imshow(img,cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fb82f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latents = vae.encoder.predict(X_train)[2]\n",
    "scatter = px.scatter_3d(x=latents[:, 0], y=latents[:, 1], z=latents[:, 2], color=target_train)\n",
    "\n",
    "scatter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea370a83",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latents = vae.encoder.predict(X_test)[2]\n",
    "scatter = px.scatter_3d(x=latents[:, 0], y=latents[:, 1], z=latents[:, 2], color=y_test)\n",
    "\n",
    "scatter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0a98e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c957f8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "s = np.random.choice(range(len(X_val)), n_samples)\n",
    "s = X_val[s]\n",
    "#s = [X_train_img[i] for i in s]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_samples, figsize=(10, 2))\n",
    "for img, ax_row in zip(s, axes.T):\n",
    "    x = vae.decoder.predict(vae.encoder.predict(img.reshape(1, 28, 28, 1))[2]).reshape(28, 28)\n",
    "    diff = x - img.reshape(28, 28)\n",
    "    error = (diff * diff).sum()\n",
    "    ax_row[0].axis('off')\n",
    "    ax_row[1].axis('off')\n",
    "    ax_row[0].imshow(img,cmap='gray', vmin=0, vmax=1)\n",
    "    ax_row[1].imshow(x, cmap='gray', vmin=0, vmax=1)\n",
    "    ax_row[1].set_title('E={:.1f}'.format(error))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350edb6c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_test_bin = y_test.copy()\n",
    "y_test_bin[y_test != 0] = 1\n",
    "y_val_bin = y_val.copy()\n",
    "y_val_bin[y_val != 0] = 1\n",
    "# Evaluate\n",
    "reconstruction = vae.decoder.predict(vae.encoder(X_val)[2])\n",
    "rerrors = (reconstruction - X_val).reshape(-1, 28*28)\n",
    "rerrors = (rerrors * rerrors).sum(axis=1)\n",
    "\n",
    "# Let's calculate scores if any anomaly is present\n",
    "if np.any(y_val_bin == 1):\n",
    "    eval = evaluate(y_val_bin.astype(int), rerrors.astype(float))\n",
    "    pr, rec, thr = eval['PR']\n",
    "    f1s = (2 * ((pr * rec)[:-1]/(pr + rec)[:-1]))\n",
    "    threshold = thr[np.argmax(f1s)]\n",
    "    print('Optimal threshold: {}'.format(threshold))\n",
    "\n",
    "    reconstruction = vae.decoder.predict(vae.encoder(X_test)[2])\n",
    "    reconstruction_error = (reconstruction - X_test).reshape(-1, 28*28)\n",
    "    reconstruction_error = (reconstruction_error * reconstruction_error).sum(axis=1)\n",
    "\n",
    "\n",
    "    classification = (reconstruction_error > threshold).astype(int)\n",
    "\n",
    "    print('Precision: {}'.format(metrics.precision_score(y_test_bin, classification)))\n",
    "    print('Recall: {}'.format(metrics.recall_score(y_test_bin, classification)))\n",
    "    print('F1: {}'.format(metrics.f1_score(y_test_bin, classification)))\n",
    "\n",
    "    metrics.confusion_matrix(y_test_bin, classification)\n",
    "else:\n",
    "    reconstruction_error = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5568d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sort Data by Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b304ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if reconstruction_error is not None:\n",
    "    combined = list(zip(X_test, reconstruction_error))\n",
    "    combined.sort(key = lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fd7f3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Show Top Autoencoder Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d7a5c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if reconstruction_error is not None:\n",
    "    n_rows = 10\n",
    "    n_cols = 10\n",
    "    n_samples = n_rows*n_cols\n",
    "\n",
    "    samples = [c[0] for c in combined[-n_samples:]]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(2*n_cols, 2*n_rows))\n",
    "    for img, ax in zip(samples, axes.reshape(-1)):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(img.reshape((28,28)), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd3f88",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Autoencoders are the most prominent reconstruction error based anomaly detection method.\n",
    "- Can provide high quality results on high dimensional data.\n",
    "- Architecture is highly adaptable to the data (fully connected, CNN, attention,...).\n",
    "- Sensitive to contamination.\n",
    "- Variational autoencoder are an important variant the improves the interpretability of the latent space.\n",
    "\n",
    "## Implementations\n",
    "- Keras: see vae.py or [here](https://keras.io/examples/generative/vae/)\n",
    "- Pytorch: [example implementation](https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb)\n",
    "- Pyro (pytorch based probabilistic programming language): [example implementation](https://pyro.ai/examples/vae.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd207a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai_presentation_last_slide.svg\" alt=\"Snow\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dab0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
